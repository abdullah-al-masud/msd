<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title>test-report.html</title>
    <link href="assets/style.css" rel="stylesheet" type="text/css"/></head>
  <body onLoad="init()">
    <script>/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this file,
 * You can obtain one at http://mozilla.org/MPL/2.0/. */


function toArray(iter) {
    if (iter === null) {
        return null;
    }
    return Array.prototype.slice.call(iter);
}

function find(selector, elem) { // eslint-disable-line no-redeclare
    if (!elem) {
        elem = document;
    }
    return elem.querySelector(selector);
}

function findAll(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return toArray(elem.querySelectorAll(selector));
}

function sortColumn(elem) {
    toggleSortStates(elem);
    const colIndex = toArray(elem.parentNode.childNodes).indexOf(elem);
    let key;
    if (elem.classList.contains('result')) {
        key = keyResult;
    } else if (elem.classList.contains('links')) {
        key = keyLink;
    } else {
        key = keyAlpha;
    }
    sortTable(elem, key(colIndex));
}

function showAllExtras() { // eslint-disable-line no-unused-vars
    findAll('.col-result').forEach(showExtras);
}

function hideAllExtras() { // eslint-disable-line no-unused-vars
    findAll('.col-result').forEach(hideExtras);
}

function showExtras(colresultElem) {
    const extras = colresultElem.parentNode.nextElementSibling;
    const expandcollapse = colresultElem.firstElementChild;
    extras.classList.remove('collapsed');
    expandcollapse.classList.remove('expander');
    expandcollapse.classList.add('collapser');
}

function hideExtras(colresultElem) {
    const extras = colresultElem.parentNode.nextElementSibling;
    const expandcollapse = colresultElem.firstElementChild;
    extras.classList.add('collapsed');
    expandcollapse.classList.remove('collapser');
    expandcollapse.classList.add('expander');
}

function showFilters() {
    let visibleString = getQueryParameter('visible') || 'all';
    visibleString = visibleString.toLowerCase();
    const checkedItems = visibleString.split(',');

    const filterItems = document.getElementsByClassName('filter');
    for (let i = 0; i < filterItems.length; i++) {
        filterItems[i].hidden = false;

        if (visibleString != 'all') {
            filterItems[i].checked = checkedItems.includes(filterItems[i].getAttribute('data-test-result'));
            filterTable(filterItems[i]);
        }
    }
}

function addCollapse() {
    // Add links for show/hide all
    const resulttable = find('table#results-table');
    const showhideall = document.createElement('p');
    showhideall.innerHTML = '<a href="javascript:showAllExtras()">Show all details</a> / ' +
                            '<a href="javascript:hideAllExtras()">Hide all details</a>';
    resulttable.parentElement.insertBefore(showhideall, resulttable);

    // Add show/hide link to each result
    findAll('.col-result').forEach(function(elem) {
        const collapsed = getQueryParameter('collapsed') || 'Passed';
        const extras = elem.parentNode.nextElementSibling;
        const expandcollapse = document.createElement('span');
        if (extras.classList.contains('collapsed')) {
            expandcollapse.classList.add('expander');
        } else if (collapsed.includes(elem.innerHTML)) {
            extras.classList.add('collapsed');
            expandcollapse.classList.add('expander');
        } else {
            expandcollapse.classList.add('collapser');
        }
        elem.appendChild(expandcollapse);

        elem.addEventListener('click', function(event) {
            if (event.currentTarget.parentNode.nextElementSibling.classList.contains('collapsed')) {
                showExtras(event.currentTarget);
            } else {
                hideExtras(event.currentTarget);
            }
        });
    });
}

function getQueryParameter(name) {
    const match = RegExp('[?&]' + name + '=([^&]*)').exec(window.location.search);
    return match && decodeURIComponent(match[1].replace(/\+/g, ' '));
}

function init () { // eslint-disable-line no-unused-vars
    resetSortHeaders();

    addCollapse();

    showFilters();

    sortColumn(find('.initial-sort'));

    findAll('.sortable').forEach(function(elem) {
        elem.addEventListener('click',
            function() {
                sortColumn(elem);
            }, false);
    });
}

function sortTable(clicked, keyFunc) {
    const rows = findAll('.results-table-row');
    const reversed = !clicked.classList.contains('asc');
    const sortedRows = sort(rows, keyFunc, reversed);
    /* Whole table is removed here because browsers acts much slower
     * when appending existing elements.
     */
    const thead = document.getElementById('results-table-head');
    document.getElementById('results-table').remove();
    const parent = document.createElement('table');
    parent.id = 'results-table';
    parent.appendChild(thead);
    sortedRows.forEach(function(elem) {
        parent.appendChild(elem);
    });
    document.getElementsByTagName('BODY')[0].appendChild(parent);
}

function sort(items, keyFunc, reversed) {
    const sortArray = items.map(function(item, i) {
        return [keyFunc(item), i];
    });

    sortArray.sort(function(a, b) {
        const keyA = a[0];
        const keyB = b[0];

        if (keyA == keyB) return 0;

        if (reversed) {
            return keyA < keyB ? 1 : -1;
        } else {
            return keyA > keyB ? 1 : -1;
        }
    });

    return sortArray.map(function(item) {
        const index = item[1];
        return items[index];
    });
}

function keyAlpha(colIndex) {
    return function(elem) {
        return elem.childNodes[1].childNodes[colIndex].firstChild.data.toLowerCase();
    };
}

function keyLink(colIndex) {
    return function(elem) {
        const dataCell = elem.childNodes[1].childNodes[colIndex].firstChild;
        return dataCell == null ? '' : dataCell.innerText.toLowerCase();
    };
}

function keyResult(colIndex) {
    return function(elem) {
        const strings = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed',
            'Skipped', 'Passed'];
        return strings.indexOf(elem.childNodes[1].childNodes[colIndex].firstChild.data);
    };
}

function resetSortHeaders() {
    findAll('.sort-icon').forEach(function(elem) {
        elem.parentNode.removeChild(elem);
    });
    findAll('.sortable').forEach(function(elem) {
        const icon = document.createElement('div');
        icon.className = 'sort-icon';
        icon.textContent = 'vvv';
        elem.insertBefore(icon, elem.firstChild);
        elem.classList.remove('desc', 'active');
        elem.classList.add('asc', 'inactive');
    });
}

function toggleSortStates(elem) {
    //if active, toggle between asc and desc
    if (elem.classList.contains('active')) {
        elem.classList.toggle('asc');
        elem.classList.toggle('desc');
    }

    //if inactive, reset all other functions and add ascending active
    if (elem.classList.contains('inactive')) {
        resetSortHeaders();
        elem.classList.remove('inactive');
        elem.classList.add('active');
    }
}

function isAllRowsHidden(value) {
    return value.hidden == false;
}

function filterTable(elem) { // eslint-disable-line no-unused-vars
    const outcomeAtt = 'data-test-result';
    const outcome = elem.getAttribute(outcomeAtt);
    const classOutcome = outcome + ' results-table-row';
    const outcomeRows = document.getElementsByClassName(classOutcome);

    for(let i = 0; i < outcomeRows.length; i++){
        outcomeRows[i].hidden = !elem.checked;
    }

    const rows = findAll('.results-table-row').filter(isAllRowsHidden);
    const allRowsHidden = rows.length == 0 ? true : false;
    const notFoundMessage = document.getElementById('not-found-message');
    notFoundMessage.hidden = !allRowsHidden;
}
</script>
    <h1>test-report.html</h1>
    <p>Report generated on 02-May-2023 at 21:50:09 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a> v3.2.0</p>
    <h2>Environment</h2>
    <table id="environment">
      <tr>
        <td>Packages</td>
        <td>{"pluggy": "1.0.0", "pytest": "7.3.1"}</td></tr>
      <tr>
        <td>Platform</td>
        <td>Linux-5.15.0-71-generic-x86_64-with-glibc2.10</td></tr>
      <tr>
        <td>Plugins</td>
        <td>{"html": "3.2.0", "metadata": "2.0.4"}</td></tr>
      <tr>
        <td>Python</td>
        <td>3.8.16</td></tr></table>
    <h2>Summary</h2>
    <p>17 tests ran in 7.08 seconds. </p>
    <p class="filter" hidden="true">(Un)check the boxes to filter the results.</p><input checked="true" class="filter" data-test-result="passed" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="passed">17 passed</span>, <input checked="true" class="filter" data-test-result="skipped" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="skipped">0 skipped</span>, <input checked="true" class="filter" data-test-result="failed" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="failed">0 failed</span>, <input checked="true" class="filter" data-test-result="error" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="error">0 errors</span>, <input checked="true" class="filter" data-test-result="xfailed" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="xfailed">0 expected failures</span>, <input checked="true" class="filter" data-test-result="xpassed" disabled="true" hidden="true" name="filter_checkbox" onChange="filterTable(this)" type="checkbox"/><span class="xpassed">0 unexpected passes</span>
    <h2>Results</h2>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable result initial-sort" col="result">Result</th>
          <th class="sortable" col="name">Test</th>
          <th class="sortable" col="duration">Duration</th>
          <th class="sortable links" col="links">Links</th></tr>
        <tr hidden="true" id="not-found-message">
          <th colspan="4">No results found. Try to check the filters</th></tr></thead>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_SplitDataset.py::test_SplitDataset_random_split</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_SplitDataset.py::test_SplitDataset_sequence_split</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_SplitDataset.py::test_SplitDataset_CV_split_only_index_True</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_SplitDataset.py::test_SplitDataset_CV_split_only_index_False</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_mlutils.py::test_define_layers</td>
          <td class="col-duration">0.02</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_mlutils.py::test_torchModel</td>
          <td class="col-duration">1.47</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>Cuda is available and will be used for training
Training will be done on these cuda devices: [0]
epoch : 0001/0080, batch : 001, train_loss : 1.2272, validation_loss : nan,  0:00:00 &lt; 0:02:41                    epoch : 0001/0080, batch : 002, train_loss : 1.1682, validation_loss : nan,  0:00:00 &lt; 0:01:20                    epoch : 0001/0080, batch : 003, train_loss : 1.1950, validation_loss : nan,  0:00:00 &lt; 0:00:53                    epoch : 0001/0080, batch : 004, train_loss : 1.1588, validation_loss : nan,  0:00:00 &lt; 0:00:40                    epoch : 0002/0080, batch : 001, train_loss : 1.1797, validation_loss : 1.1077,  0:00:00 &lt; 0:00:32                    epoch : 0002/0080, batch : 002, train_loss : 1.1737, validation_loss : 1.1077,  0:00:00 &lt; 0:00:26                    epoch : 0002/0080, batch : 003, train_loss : 1.1313, validation_loss : 1.1077,  0:00:00 &lt; 0:00:22                    epoch : 0002/0080, batch : 004, train_loss : 1.1760, validation_loss : 1.1077,  0:00:00 &lt; 0:00:19                    epoch : 0003/0080, batch : 001, train_loss : 1.1561, validation_loss : 1.0920,  0:00:00 &lt; 0:00:17                    epoch : 0003/0080, batch : 002, train_loss : 1.1410, validation_loss : 1.0920,  0:00:00 &lt; 0:00:15                    epoch : 0003/0080, batch : 003, train_loss : 1.1079, validation_loss : 1.0920,  0:00:00 &lt; 0:00:14                    epoch : 0003/0080, batch : 004, train_loss : 1.1720, validation_loss : 1.0920,  0:00:00 &lt; 0:00:13                    epoch : 0004/0080, batch : 001, train_loss : 1.1268, validation_loss : 1.0789,  0:00:00 &lt; 0:00:12                    epoch : 0004/0080, batch : 002, train_loss : 1.1031, validation_loss : 1.0789,  0:00:00 &lt; 0:00:11                    epoch : 0004/0080, batch : 003, train_loss : 1.1268, validation_loss : 1.0789,  0:00:00 &lt; 0:00:10                    epoch : 0004/0080, batch : 004, train_loss : 1.1407, validation_loss : 1.0789,  0:00:00 &lt; 0:00:09                    epoch : 0005/0080, batch : 001, train_loss : 1.1221, validation_loss : 1.0677,  0:00:00 &lt; 0:00:09                    epoch : 0005/0080, batch : 002, train_loss : 1.1216, validation_loss : 1.0677,  0:00:00 &lt; 0:00:08                    epoch : 0005/0080, batch : 003, train_loss : 1.0891, validation_loss : 1.0677,  0:00:00 &lt; 0:00:08                    epoch : 0005/0080, batch : 004, train_loss : 1.0941, validation_loss : 1.0677,  0:00:00 &lt; 0:00:07                    epoch : 0006/0080, batch : 001, train_loss : 1.0816, validation_loss : 1.0586,  0:00:00 &lt; 0:00:07                    epoch : 0006/0080, batch : 002, train_loss : 1.0840, validation_loss : 1.0586,  0:00:00 &lt; 0:00:07                    epoch : 0006/0080, batch : 003, train_loss : 1.0958, validation_loss : 1.0586,  0:00:00 &lt; 0:00:06                    epoch : 0006/0080, batch : 004, train_loss : 1.1154, validation_loss : 1.0586,  0:00:00 &lt; 0:00:06                    epoch : 0007/0080, batch : 001, train_loss : 1.0737, validation_loss : 1.0498,  0:00:00 &lt; 0:00:06                    epoch : 0007/0080, batch : 002, train_loss : 1.0980, validation_loss : 1.0498,  0:00:00 &lt; 0:00:05                    epoch : 0007/0080, batch : 003, train_loss : 1.0672, validation_loss : 1.0498,  0:00:00 &lt; 0:00:05                    epoch : 0007/0080, batch : 004, train_loss : 1.0763, validation_loss : 1.0498,  0:00:00 &lt; 0:00:05                    epoch : 0008/0080, batch : 001, train_loss : 1.0931, validation_loss : 1.0421,  0:00:00 &lt; 0:00:05                    epoch : 0008/0080, batch : 002, train_loss : 1.0591, validation_loss : 1.0421,  0:00:00 &lt; 0:00:05                    epoch : 0008/0080, batch : 003, train_loss : 1.0786, validation_loss : 1.0421,  0:00:00 &lt; 0:00:04                    epoch : 0008/0080, batch : 004, train_loss : 1.0293, validation_loss : 1.0421,  0:00:00 &lt; 0:00:04                    epoch : 0009/0080, batch : 001, train_loss : 1.0689, validation_loss : 1.0353,  0:00:00 &lt; 0:00:04                    epoch : 0009/0080, batch : 002, train_loss : 1.0470, validation_loss : 1.0353,  0:00:00 &lt; 0:00:04                    epoch : 0009/0080, batch : 003, train_loss : 1.0595, validation_loss : 1.0353,  0:00:00 &lt; 0:00:04                    epoch : 0009/0080, batch : 004, train_loss : 1.0441, validation_loss : 1.0353,  0:00:00 &lt; 0:00:04                    epoch : 0010/0080, batch : 001, train_loss : 1.0456, validation_loss : 1.0275,  0:00:00 &lt; 0:00:04                    epoch : 0010/0080, batch : 002, train_loss : 1.0628, validation_loss : 1.0275,  0:00:00 &lt; 0:00:03                    epoch : 0010/0080, batch : 003, train_loss : 1.0271, validation_loss : 1.0275,  0:00:00 &lt; 0:00:03                    epoch : 0010/0080, batch : 004, train_loss : 1.0462, validation_loss : 1.0275,  0:00:00 &lt; 0:00:03                    epoch : 0011/0080, batch : 001, train_loss : 1.0467, validation_loss : 1.0191,  0:00:00 &lt; 0:00:03                    epoch : 0011/0080, batch : 002, train_loss : 1.0229, validation_loss : 1.0191,  0:00:00 &lt; 0:00:03                    epoch : 0011/0080, batch : 003, train_loss : 1.0512, validation_loss : 1.0191,  0:00:00 &lt; 0:00:03                    epoch : 0011/0080, batch : 004, train_loss : 1.0186, validation_loss : 1.0191,  0:00:00 &lt; 0:00:03                    epoch : 0012/0080, batch : 001, train_loss : 1.0382, validation_loss : 1.0128,  0:00:00 &lt; 0:00:03                    epoch : 0012/0080, batch : 002, train_loss : 1.0073, validation_loss : 1.0128,  0:00:00 &lt; 0:00:03                    epoch : 0012/0080, batch : 003, train_loss : 1.0194, validation_loss : 1.0128,  0:00:00 &lt; 0:00:03                    epoch : 0012/0080, batch : 004, train_loss : 1.0461, validation_loss : 1.0128,  0:00:00 &lt; 0:00:03                    epoch : 0013/0080, batch : 001, train_loss : 1.0185, validation_loss : 1.0055,  0:00:00 &lt; 0:00:03                    epoch : 0013/0080, batch : 002, train_loss : 1.0152, validation_loss : 1.0055,  0:00:00 &lt; 0:00:02                    epoch : 0013/0080, batch : 003, train_loss : 1.0189, validation_loss : 1.0055,  0:00:00 &lt; 0:00:02                    epoch : 0013/0080, batch : 004, train_loss : 1.0168, validation_loss : 1.0055,  0:00:00 &lt; 0:00:02                    epoch : 0014/0080, batch : 001, train_loss : 1.0095, validation_loss : 0.9972,  0:00:00 &lt; 0:00:02                    epoch : 0014/0080, batch : 002, train_loss : 1.0171, validation_loss : 0.9972,  0:00:00 &lt; 0:00:02                    epoch : 0014/0080, batch : 003, train_loss : 1.0168, validation_loss : 0.9972,  0:00:00 &lt; 0:00:02                    epoch : 0014/0080, batch : 004, train_loss : 0.9878, validation_loss : 0.9972,  0:00:00 &lt; 0:00:02                    epoch : 0015/0080, batch : 001, train_loss : 1.0106, validation_loss : 0.9889,  0:00:00 &lt; 0:00:02                    epoch : 0015/0080, batch : 002, train_loss : 1.0009, validation_loss : 0.9889,  0:00:00 &lt; 0:00:02                    epoch : 0015/0080, batch : 003, train_loss : 0.9971, validation_loss : 0.9889,  0:00:00 &lt; 0:00:02                    epoch : 0015/0080, batch : 004, train_loss : 0.9905, validation_loss : 0.9889,  0:00:00 &lt; 0:00:02                    epoch : 0016/0080, batch : 001, train_loss : 1.0012, validation_loss : 0.9810,  0:00:00 &lt; 0:00:02                    epoch : 0016/0080, batch : 002, train_loss : 0.9938, validation_loss : 0.9810,  0:00:00 &lt; 0:00:02                    epoch : 0016/0080, batch : 003, train_loss : 0.9866, validation_loss : 0.9810,  0:00:00 &lt; 0:00:02                    epoch : 0016/0080, batch : 004, train_loss : 0.9835, validation_loss : 0.9810,  0:00:00 &lt; 0:00:02                    epoch : 0017/0080, batch : 001, train_loss : 0.9736, validation_loss : 0.9734,  0:00:00 &lt; 0:00:02                    epoch : 0017/0080, batch : 002, train_loss : 0.9869, validation_loss : 0.9734,  0:00:00 &lt; 0:00:02                    epoch : 0017/0080, batch : 003, train_loss : 0.9779, validation_loss : 0.9734,  0:00:00 &lt; 0:00:02                    epoch : 0017/0080, batch : 004, train_loss : 0.9998, validation_loss : 0.9734,  0:00:00 &lt; 0:00:02                    epoch : 0018/0080, batch : 001, train_loss : 0.9805, validation_loss : 0.9651,  0:00:00 &lt; 0:00:02                    epoch : 0018/0080, batch : 002, train_loss : 0.9676, validation_loss : 0.9651,  0:00:00 &lt; 0:00:01                    epoch : 0018/0080, batch : 003, train_loss : 0.9845, validation_loss : 0.9651,  0:00:00 &lt; 0:00:01                    epoch : 0018/0080, batch : 004, train_loss : 0.9670, validation_loss : 0.9651,  0:00:00 &lt; 0:00:01                    epoch : 0019/0080, batch : 001, train_loss : 0.9630, validation_loss : 0.9579,  0:00:00 &lt; 0:00:01                    epoch : 0019/0080, batch : 002, train_loss : 0.9720, validation_loss : 0.9579,  0:00:00 &lt; 0:00:01                    epoch : 0019/0080, batch : 003, train_loss : 0.9795, validation_loss : 0.9579,  0:00:00 &lt; 0:00:01                    epoch : 0019/0080, batch : 004, train_loss : 0.9513, validation_loss : 0.9579,  0:00:00 &lt; 0:00:01                    epoch : 0020/0080, batch : 001, train_loss : 0.9495, validation_loss : 0.9500,  0:00:00 &lt; 0:00:01                    epoch : 0020/0080, batch : 002, train_loss : 0.9729, validation_loss : 0.9500,  0:00:00 &lt; 0:00:01                    epoch : 0020/0080, batch : 003, train_loss : 0.9692, validation_loss : 0.9500,  0:00:00 &lt; 0:00:01                    epoch : 0020/0080, batch : 004, train_loss : 0.9417, validation_loss : 0.9500,  0:00:00 &lt; 0:00:01                    epoch : 0021/0080, batch : 001, train_loss : 0.9636, validation_loss : 0.9402,  0:00:00 &lt; 0:00:01                    epoch : 0021/0080, batch : 002, train_loss : 0.9539, validation_loss : 0.9402,  0:00:00 &lt; 0:00:01                    epoch : 0021/0080, batch : 003, train_loss : 0.9309, validation_loss : 0.9402,  0:00:00 &lt; 0:00:01                    epoch : 0021/0080, batch : 004, train_loss : 0.9525, validation_loss : 0.9402,  0:00:00 &lt; 0:00:01                    epoch : 0022/0080, batch : 001, train_loss : 0.9520, validation_loss : 0.9311,  0:00:00 &lt; 0:00:01                    epoch : 0022/0080, batch : 002, train_loss : 0.9423, validation_loss : 0.9311,  0:00:00 &lt; 0:00:01                    epoch : 0022/0080, batch : 003, train_loss : 0.9415, validation_loss : 0.9311,  0:00:00 &lt; 0:00:01                    epoch : 0022/0080, batch : 004, train_loss : 0.9253, validation_loss : 0.9311,  0:00:00 &lt; 0:00:01                    epoch : 0023/0080, batch : 001, train_loss : 0.9446, validation_loss : 0.9218,  0:00:00 &lt; 0:00:01                    epoch : 0023/0080, batch : 002, train_loss : 0.9397, validation_loss : 0.9218,  0:00:00 &lt; 0:00:01                    epoch : 0023/0080, batch : 003, train_loss : 0.9249, validation_loss : 0.9218,  0:00:00 &lt; 0:00:01                    epoch : 0023/0080, batch : 004, train_loss : 0.9134, validation_loss : 0.9218,  0:00:00 &lt; 0:00:01                    epoch : 0024/0080, batch : 001, train_loss : 0.9309, validation_loss : 0.9125,  0:00:00 &lt; 0:00:01                    epoch : 0024/0080, batch : 002, train_loss : 0.9151, validation_loss : 0.9125,  0:00:00 &lt; 0:00:01                    epoch : 0024/0080, batch : 003, train_loss : 0.9402, validation_loss : 0.9125,  0:00:00 &lt; 0:00:01                    epoch : 0024/0080, batch : 004, train_loss : 0.8988, validation_loss : 0.9125,  0:00:00 &lt; 0:00:01                    epoch : 0025/0080, batch : 001, train_loss : 0.9152, validation_loss : 0.9026,  0:00:00 &lt; 0:00:01                    epoch : 0025/0080, batch : 002, train_loss : 0.9137, validation_loss : 0.9026,  0:00:00 &lt; 0:00:01                    epoch : 0025/0080, batch : 003, train_loss : 0.9165, validation_loss : 0.9026,  0:00:00 &lt; 0:00:01                    epoch : 0025/0080, batch : 004, train_loss : 0.9069, validation_loss : 0.9026,  0:00:00 &lt; 0:00:01                    epoch : 0026/0080, batch : 001, train_loss : 0.9088, validation_loss : 0.8920,  0:00:00 &lt; 0:00:01                    epoch : 0026/0080, batch : 002, train_loss : 0.9141, validation_loss : 0.8920,  0:00:00 &lt; 0:00:01                    epoch : 0026/0080, batch : 003, train_loss : 0.8903, validation_loss : 0.8920,  0:00:00 &lt; 0:00:01                    epoch : 0026/0080, batch : 004, train_loss : 0.9009, validation_loss : 0.8920,  0:00:00 &lt; 0:00:01                    epoch : 0027/0080, batch : 001, train_loss : 0.9061, validation_loss : 0.8826,  0:00:00 &lt; 0:00:01                    epoch : 0027/0080, batch : 002, train_loss : 0.9087, validation_loss : 0.8826,  0:00:00 &lt; 0:00:01                    epoch : 0027/0080, batch : 003, train_loss : 0.8826, validation_loss : 0.8826,  0:00:00 &lt; 0:00:01                    epoch : 0027/0080, batch : 004, train_loss : 0.8768, validation_loss : 0.8826,  0:00:00 &lt; 0:00:01                    epoch : 0028/0080, batch : 001, train_loss : 0.8742, validation_loss : 0.8747,  0:00:00 &lt; 0:00:01                    epoch : 0028/0080, batch : 002, train_loss : 0.9087, validation_loss : 0.8747,  0:00:00 &lt; 0:00:01                    epoch : 0028/0080, batch : 003, train_loss : 0.8964, validation_loss : 0.8747,  0:00:00 &lt; 0:00:01                    epoch : 0028/0080, batch : 004, train_loss : 0.8542, validation_loss : 0.8747,  0:00:00 &lt; 0:00:01                    epoch : 0029/0080, batch : 001, train_loss : 0.8823, validation_loss : 0.8657,  0:00:00 &lt; 0:00:01                    epoch : 0029/0080, batch : 002, train_loss : 0.8674, validation_loss : 0.8657,  0:00:00 &lt; 0:00:01                    epoch : 0029/0080, batch : 003, train_loss : 0.8755, validation_loss : 0.8657,  0:00:00 &lt; 0:00:01                    epoch : 0029/0080, batch : 004, train_loss : 0.8817, validation_loss : 0.8657,  0:00:00 &lt; 0:00:01                    epoch : 0030/0080, batch : 001, train_loss : 0.8695, validation_loss : 0.8565,  0:00:00 &lt; 0:00:01                    epoch : 0030/0080, batch : 002, train_loss : 0.8815, validation_loss : 0.8565,  0:00:00 &lt; 0:00:01                    epoch : 0030/0080, batch : 003, train_loss : 0.8624, validation_loss : 0.8565,  0:00:00 &lt; 0:00:01                    epoch : 0030/0080, batch : 004, train_loss : 0.8518, validation_loss : 0.8565,  0:00:00 &lt; 0:00:00                    epoch : 0031/0080, batch : 001, train_loss : 0.8703, validation_loss : 0.8480,  0:00:00 &lt; 0:00:00                    epoch : 0031/0080, batch : 002, train_loss : 0.8595, validation_loss : 0.8480,  0:00:00 &lt; 0:00:00                    epoch : 0031/0080, batch : 003, train_loss : 0.8436, validation_loss : 0.8480,  0:00:00 &lt; 0:00:00                    epoch : 0031/0080, batch : 004, train_loss : 0.8598, validation_loss : 0.8480,  0:00:00 &lt; 0:00:00                    epoch : 0032/0080, batch : 001, train_loss : 0.8543, validation_loss : 0.8395,  0:00:00 &lt; 0:00:00                    epoch : 0032/0080, batch : 002, train_loss : 0.8764, validation_loss : 0.8395,  0:00:00 &lt; 0:00:00                    epoch : 0032/0080, batch : 003, train_loss : 0.8398, validation_loss : 0.8395,  0:00:00 &lt; 0:00:00                    epoch : 0032/0080, batch : 004, train_loss : 0.8185, validation_loss : 0.8395,  0:00:00 &lt; 0:00:00                    epoch : 0033/0080, batch : 001, train_loss : 0.8430, validation_loss : 0.8314,  0:00:00 &lt; 0:00:00                    epoch : 0033/0080, batch : 002, train_loss : 0.8379, validation_loss : 0.8314,  0:00:00 &lt; 0:00:00                    epoch : 0033/0080, batch : 003, train_loss : 0.8384, validation_loss : 0.8314,  0:00:00 &lt; 0:00:00                    epoch : 0033/0080, batch : 004, train_loss : 0.8416, validation_loss : 0.8314,  0:00:00 &lt; 0:00:00                    epoch : 0034/0080, batch : 001, train_loss : 0.8096, validation_loss : 0.8223,  0:00:00 &lt; 0:00:00                    epoch : 0034/0080, batch : 002, train_loss : 0.8661, validation_loss : 0.8223,  0:00:00 &lt; 0:00:00                    epoch : 0034/0080, batch : 003, train_loss : 0.8342, validation_loss : 0.8223,  0:00:00 &lt; 0:00:00                    epoch : 0034/0080, batch : 004, train_loss : 0.8096, validation_loss : 0.8223,  0:00:00 &lt; 0:00:00                    epoch : 0035/0080, batch : 001, train_loss : 0.8055, validation_loss : 0.8147,  0:00:00 &lt; 0:00:00                    epoch : 0035/0080, batch : 002, train_loss : 0.8133, validation_loss : 0.8147,  0:00:00 &lt; 0:00:00                    epoch : 0035/0080, batch : 003, train_loss : 0.8108, validation_loss : 0.8147,  0:00:00 &lt; 0:00:00                    epoch : 0035/0080, batch : 004, train_loss : 0.8721, validation_loss : 0.8147,  0:00:00 &lt; 0:00:00                    epoch : 0036/0080, batch : 001, train_loss : 0.7954, validation_loss : 0.8057,  0:00:00 &lt; 0:00:00                    epoch : 0036/0080, batch : 002, train_loss : 0.7987, validation_loss : 0.8057,  0:00:00 &lt; 0:00:00                    epoch : 0036/0080, batch : 003, train_loss : 0.8373, validation_loss : 0.8057,  0:00:00 &lt; 0:00:00                    epoch : 0036/0080, batch : 004, train_loss : 0.8234, validation_loss : 0.8057,  0:00:00 &lt; 0:00:00                    epoch : 0037/0080, batch : 001, train_loss : 0.8060, validation_loss : 0.7960,  0:00:00 &lt; 0:00:00                    epoch : 0037/0080, batch : 002, train_loss : 0.8073, validation_loss : 0.7960,  0:00:00 &lt; 0:00:00                    epoch : 0037/0080, batch : 003, train_loss : 0.8231, validation_loss : 0.7960,  0:00:00 &lt; 0:00:00                    epoch : 0037/0080, batch : 004, train_loss : 0.7675, validation_loss : 0.7960,  0:00:00 &lt; 0:00:00                    epoch : 0038/0080, batch : 001, train_loss : 0.7902, validation_loss : 0.7864,  0:00:00 &lt; 0:00:00                    epoch : 0038/0080, batch : 002, train_loss : 0.8042, validation_loss : 0.7864,  0:00:00 &lt; 0:00:00                    epoch : 0038/0080, batch : 003, train_loss : 0.7826, validation_loss : 0.7864,  0:00:00 &lt; 0:00:00                    epoch : 0038/0080, batch : 004, train_loss : 0.7987, validation_loss : 0.7864,  0:00:00 &lt; 0:00:00                    epoch : 0039/0080, batch : 001, train_loss : 0.7730, validation_loss : 0.7760,  0:00:00 &lt; 0:00:00                    epoch : 0039/0080, batch : 002, train_loss : 0.7917, validation_loss : 0.7760,  0:00:00 &lt; 0:00:00                    epoch : 0039/0080, batch : 003, train_loss : 0.7662, validation_loss : 0.7760,  0:00:00 &lt; 0:00:00                    epoch : 0039/0080, batch : 004, train_loss : 0.8119, validation_loss : 0.7760,  0:00:00 &lt; 0:00:00                    epoch : 0040/0080, batch : 001, train_loss : 0.7791, validation_loss : 0.7668,  0:00:00 &lt; 0:00:00                    epoch : 0040/0080, batch : 002, train_loss : 0.7989, validation_loss : 0.7668,  0:00:00 &lt; 0:00:00                    epoch : 0040/0080, batch : 003, train_loss : 0.7512, validation_loss : 0.7668,  0:00:00 &lt; 0:00:00                    epoch : 0040/0080, batch : 004, train_loss : 0.7661, validation_loss : 0.7668,  0:00:00 &lt; 0:00:00                    epoch : 0041/0080, batch : 001, train_loss : 0.7578, validation_loss : 0.7579,  0:00:00 &lt; 0:00:00                    epoch : 0041/0080, batch : 002, train_loss : 0.7378, validation_loss : 0.7579,  0:00:00 &lt; 0:00:00                    epoch : 0041/0080, batch : 003, train_loss : 0.8267, validation_loss : 0.7579,  0:00:00 &lt; 0:00:00                    epoch : 0041/0080, batch : 004, train_loss : 0.7317, validation_loss : 0.7579,  0:00:00 &lt; 0:00:00                    epoch : 0042/0080, batch : 001, train_loss : 0.7892, validation_loss : 0.7484,  0:00:00 &lt; 0:00:00                    epoch : 0042/0080, batch : 002, train_loss : 0.7520, validation_loss : 0.7484,  0:00:00 &lt; 0:00:00                    epoch : 0042/0080, batch : 003, train_loss : 0.7286, validation_loss : 0.7484,  0:00:00 &lt; 0:00:00                    epoch : 0042/0080, batch : 004, train_loss : 0.7563, validation_loss : 0.7484,  0:00:00 &lt; 0:00:00                    epoch : 0043/0080, batch : 001, train_loss : 0.8123, validation_loss : 0.7394,  0:00:00 &lt; 0:00:00                    epoch : 0043/0080, batch : 002, train_loss : 0.7680, validation_loss : 0.7394,  0:00:00 &lt; 0:00:00                    epoch : 0043/0080, batch : 003, train_loss : 0.7001, validation_loss : 0.7394,  0:00:00 &lt; 0:00:00                    epoch : 0043/0080, batch : 004, train_loss : 0.6990, validation_loss : 0.7394,  0:00:00 &lt; 0:00:00                    epoch : 0044/0080, batch : 001, train_loss : 0.7739, validation_loss : 0.7316,  0:00:00 &lt; 0:00:00                    epoch : 0044/0080, batch : 002, train_loss : 0.7376, validation_loss : 0.7316,  0:00:00 &lt; 0:00:00                    epoch : 0044/0080, batch : 003, train_loss : 0.7232, validation_loss : 0.7316,  0:00:00 &lt; 0:00:00                    epoch : 0044/0080, batch : 004, train_loss : 0.7159, validation_loss : 0.7316,  0:00:00 &lt; 0:00:00                    epoch : 0045/0080, batch : 001, train_loss : 0.7371, validation_loss : 0.7232,  0:00:00 &lt; 0:00:00                    epoch : 0045/0080, batch : 002, train_loss : 0.7239, validation_loss : 0.7232,  0:00:00 &lt; 0:00:00                    epoch : 0045/0080, batch : 003, train_loss : 0.7183, validation_loss : 0.7232,  0:00:00 &lt; 0:00:00                    epoch : 0045/0080, batch : 004, train_loss : 0.7470, validation_loss : 0.7232,  0:00:00 &lt; 0:00:00                    epoch : 0046/0080, batch : 001, train_loss : 0.7345, validation_loss : 0.7142,  0:00:00 &lt; 0:00:00                    epoch : 0046/0080, batch : 002, train_loss : 0.6850, validation_loss : 0.7142,  0:00:00 &lt; 0:00:00                    epoch : 0046/0080, batch : 003, train_loss : 0.7383, validation_loss : 0.7142,  0:00:00 &lt; 0:00:00                    epoch : 0046/0080, batch : 004, train_loss : 0.7326, validation_loss : 0.7142,  0:00:00 &lt; 0:00:00                    epoch : 0047/0080, batch : 001, train_loss : 0.7364, validation_loss : 0.7052,  0:00:00 &lt; 0:00:00                    epoch : 0047/0080, batch : 002, train_loss : 0.6750, validation_loss : 0.7052,  0:00:00 &lt; 0:00:00                    epoch : 0047/0080, batch : 003, train_loss : 0.6819, validation_loss : 0.7052,  0:00:00 &lt; 0:00:00                    epoch : 0047/0080, batch : 004, train_loss : 0.7750, validation_loss : 0.7052,  0:00:00 &lt; 0:00:00                    epoch : 0048/0080, batch : 001, train_loss : 0.7059, validation_loss : 0.6969,  0:00:00 &lt; 0:00:00                    epoch : 0048/0080, batch : 002, train_loss : 0.7337, validation_loss : 0.6969,  0:00:00 &lt; 0:00:00                    epoch : 0048/0080, batch : 003, train_loss : 0.7240, validation_loss : 0.6969,  0:00:00 &lt; 0:00:00                    epoch : 0048/0080, batch : 004, train_loss : 0.6307, validation_loss : 0.6969,  0:00:00 &lt; 0:00:00                    epoch : 0049/0080, batch : 001, train_loss : 0.6241, validation_loss : 0.6879,  0:00:00 &lt; 0:00:00                    epoch : 0049/0080, batch : 002, train_loss : 0.7306, validation_loss : 0.6879,  0:00:00 &lt; 0:00:00                    epoch : 0049/0080, batch : 003, train_loss : 0.7427, validation_loss : 0.6879,  0:00:00 &lt; 0:00:00                    epoch : 0049/0080, batch : 004, train_loss : 0.6756, validation_loss : 0.6879,  0:00:00 &lt; 0:00:00                    epoch : 0050/0080, batch : 001, train_loss : 0.7305, validation_loss : 0.6781,  0:00:00 &lt; 0:00:00                    epoch : 0050/0080, batch : 002, train_loss : 0.6108, validation_loss : 0.6781,  0:00:00 &lt; 0:00:00                    epoch : 0050/0080, batch : 003, train_loss : 0.6842, validation_loss : 0.6781,  0:00:00 &lt; 0:00:00                    epoch : 0050/0080, batch : 004, train_loss : 0.7268, validation_loss : 0.6781,  0:00:00 &lt; 0:00:00                    epoch : 0051/0080, batch : 001, train_loss : 0.6566, validation_loss : 0.6680,  0:00:00 &lt; 0:00:00                    epoch : 0051/0080, batch : 002, train_loss : 0.6747, validation_loss : 0.6680,  0:00:00 &lt; 0:00:00                    epoch : 0051/0080, batch : 003, train_loss : 0.6697, validation_loss : 0.6680,  0:00:00 &lt; 0:00:00                    epoch : 0051/0080, batch : 004, train_loss : 0.7164, validation_loss : 0.6680,  0:00:00 &lt; 0:00:00                    epoch : 0052/0080, batch : 001, train_loss : 0.7063, validation_loss : 0.6596,  0:00:00 &lt; 0:00:00                    epoch : 0052/0080, batch : 002, train_loss : 0.6166, validation_loss : 0.6596,  0:00:00 &lt; 0:00:00                    epoch : 0052/0080, batch : 003, train_loss : 0.6584, validation_loss : 0.6596,  0:00:00 &lt; 0:00:00                    epoch : 0052/0080, batch : 004, train_loss : 0.7026, validation_loss : 0.6596,  0:00:00 &lt; 0:00:00                    epoch : 0053/0080, batch : 001, train_loss : 0.6399, validation_loss : 0.6512,  0:00:00 &lt; 0:00:00                    epoch : 0053/0080, batch : 002, train_loss : 0.7084, validation_loss : 0.6512,  0:00:00 &lt; 0:00:00                    epoch : 0053/0080, batch : 003, train_loss : 0.6592, validation_loss : 0.6512,  0:00:00 &lt; 0:00:00                    epoch : 0053/0080, batch : 004, train_loss : 0.6270, validation_loss : 0.6512,  0:00:00 &lt; 0:00:00                    epoch : 0054/0080, batch : 001, train_loss : 0.6607, validation_loss : 0.6445,  0:00:00 &lt; 0:00:00                    epoch : 0054/0080, batch : 002, train_loss : 0.6359, validation_loss : 0.6445,  0:00:00 &lt; 0:00:00                    epoch : 0054/0080, batch : 003, train_loss : 0.6596, validation_loss : 0.6445,  0:00:00 &lt; 0:00:00                    epoch : 0054/0080, batch : 004, train_loss : 0.6544, validation_loss : 0.6445,  0:00:00 &lt; 0:00:00                    epoch : 0055/0080, batch : 001, train_loss : 0.6598, validation_loss : 0.6380,  0:00:00 &lt; 0:00:00                    epoch : 0055/0080, batch : 002, train_loss : 0.5911, validation_loss : 0.6380,  0:00:00 &lt; 0:00:00                    epoch : 0055/0080, batch : 003, train_loss : 0.6966, validation_loss : 0.6380,  0:00:00 &lt; 0:00:00                    epoch : 0055/0080, batch : 004, train_loss : 0.6271, validation_loss : 0.6380,  0:00:00 &lt; 0:00:00                    epoch : 0056/0080, batch : 001, train_loss : 0.6521, validation_loss : 0.6311,  0:00:00 &lt; 0:00:00                    epoch : 0056/0080, batch : 002, train_loss : 0.6378, validation_loss : 0.6311,  0:00:00 &lt; 0:00:00                    epoch : 0056/0080, batch : 003, train_loss : 0.6175, validation_loss : 0.6311,  0:00:00 &lt; 0:00:00                    epoch : 0056/0080, batch : 004, train_loss : 0.6423, validation_loss : 0.6311,  0:00:00 &lt; 0:00:00                    epoch : 0057/0080, batch : 001, train_loss : 0.6292, validation_loss : 0.6243,  0:00:00 &lt; 0:00:00                    epoch : 0057/0080, batch : 002, train_loss : 0.6142, validation_loss : 0.6243,  0:00:00 &lt; 0:00:00                    epoch : 0057/0080, batch : 003, train_loss : 0.6396, validation_loss : 0.6243,  0:00:00 &lt; 0:00:00                    epoch : 0057/0080, batch : 004, train_loss : 0.6373, validation_loss : 0.6243,  0:00:00 &lt; 0:00:00                    epoch : 0058/0080, batch : 001, train_loss : 0.5911, validation_loss : 0.6170,  0:00:00 &lt; 0:00:00                    epoch : 0058/0080, batch : 002, train_loss : 0.6124, validation_loss : 0.6170,  0:00:00 &lt; 0:00:00                    epoch : 0058/0080, batch : 003, train_loss : 0.6821, validation_loss : 0.6170,  0:00:00 &lt; 0:00:00                    epoch : 0058/0080, batch : 004, train_loss : 0.5964, validation_loss : 0.6170,  0:00:00 &lt; 0:00:00                    epoch : 0059/0080, batch : 001, train_loss : 0.6185, validation_loss : 0.6091,  0:00:00 &lt; 0:00:00                    epoch : 0059/0080, batch : 002, train_loss : 0.5799, validation_loss : 0.6091,  0:00:00 &lt; 0:00:00                    epoch : 0059/0080, batch : 003, train_loss : 0.6172, validation_loss : 0.6091,  0:00:00 &lt; 0:00:00                    epoch : 0059/0080, batch : 004, train_loss : 0.6527, validation_loss : 0.6091,  0:00:00 &lt; 0:00:00                    epoch : 0060/0080, batch : 001, train_loss : 0.5912, validation_loss : 0.6018,  0:00:00 &lt; 0:00:00                    epoch : 0060/0080, batch : 002, train_loss : 0.5904, validation_loss : 0.6018,  0:00:00 &lt; 0:00:00                    epoch : 0060/0080, batch : 003, train_loss : 0.6270, validation_loss : 0.6018,  0:00:00 &lt; 0:00:00                    epoch : 0060/0080, batch : 004, train_loss : 0.6262, validation_loss : 0.6018,  0:00:00 &lt; 0:00:00                    epoch : 0061/0080, batch : 001, train_loss : 0.6514, validation_loss : 0.5948,  0:00:00 &lt; 0:00:00                    epoch : 0061/0080, batch : 002, train_loss : 0.5711, validation_loss : 0.5948,  0:00:00 &lt; 0:00:00                    epoch : 0061/0080, batch : 003, train_loss : 0.5811, validation_loss : 0.5948,  0:00:00 &lt; 0:00:00                    epoch : 0061/0080, batch : 004, train_loss : 0.5979, validation_loss : 0.5948,  0:00:00 &lt; 0:00:00                    epoch : 0062/0080, batch : 001, train_loss : 0.6922, validation_loss : 0.5887,  0:00:00 &lt; 0:00:00                    epoch : 0062/0080, batch : 002, train_loss : 0.5500, validation_loss : 0.5887,  0:00:00 &lt; 0:00:00                    epoch : 0062/0080, batch : 003, train_loss : 0.5178, validation_loss : 0.5887,  0:00:00 &lt; 0:00:00                    epoch : 0062/0080, batch : 004, train_loss : 0.6228, validation_loss : 0.5887,  0:00:00 &lt; 0:00:00                    epoch : 0063/0080, batch : 001, train_loss : 0.5503, validation_loss : 0.5829,  0:00:00 &lt; 0:00:00                    epoch : 0063/0080, batch : 002, train_loss : 0.5905, validation_loss : 0.5829,  0:00:00 &lt; 0:00:00                    epoch : 0063/0080, batch : 003, train_loss : 0.6469, validation_loss : 0.5829,  0:00:00 &lt; 0:00:00                    epoch : 0063/0080, batch : 004, train_loss : 0.5519, validation_loss : 0.5829,  0:00:00 &lt; 0:00:00                    epoch : 0064/0080, batch : 001, train_loss : 0.5869, validation_loss : 0.5767,  0:00:00 &lt; 0:00:00                    epoch : 0064/0080, batch : 002, train_loss : 0.5941, validation_loss : 0.5767,  0:00:00 &lt; 0:00:00                    epoch : 0064/0080, batch : 003, train_loss : 0.5663, validation_loss : 0.5767,  0:00:00 &lt; 0:00:00                    epoch : 0064/0080, batch : 004, train_loss : 0.5733, validation_loss : 0.5767,  0:00:00 &lt; 0:00:00                    epoch : 0065/0080, batch : 001, train_loss : 0.6112, validation_loss : 0.5682,  0:00:00 &lt; 0:00:00                    epoch : 0065/0080, batch : 002, train_loss : 0.5617, validation_loss : 0.5682,  0:00:00 &lt; 0:00:00                    epoch : 0065/0080, batch : 003, train_loss : 0.5685, validation_loss : 0.5682,  0:00:00 &lt; 0:00:00                    epoch : 0065/0080, batch : 004, train_loss : 0.5476, validation_loss : 0.5682,  0:00:00 &lt; 0:00:00                    epoch : 0066/0080, batch : 001, train_loss : 0.5843, validation_loss : 0.5613,  0:00:00 &lt; 0:00:00                    epoch : 0066/0080, batch : 002, train_loss : 0.5505, validation_loss : 0.5613,  0:00:00 &lt; 0:00:00                    epoch : 0066/0080, batch : 003, train_loss : 0.5276, validation_loss : 0.5613,  0:00:00 &lt; 0:00:00                    epoch : 0066/0080, batch : 004, train_loss : 0.6208, validation_loss : 0.5613,  0:00:00 &lt; 0:00:00                    epoch : 0067/0080, batch : 001, train_loss : 0.5626, validation_loss : 0.5559,  0:00:00 &lt; 0:00:00                    epoch : 0067/0080, batch : 002, train_loss : 0.5445, validation_loss : 0.5559,  0:00:00 &lt; 0:00:00                    epoch : 0067/0080, batch : 003, train_loss : 0.5859, validation_loss : 0.5559,  0:00:00 &lt; 0:00:00                    epoch : 0067/0080, batch : 004, train_loss : 0.5484, validation_loss : 0.5559,  0:00:00 &lt; 0:00:00                    epoch : 0068/0080, batch : 001, train_loss : 0.5535, validation_loss : 0.5505,  0:00:00 &lt; 0:00:00                    epoch : 0068/0080, batch : 002, train_loss : 0.6048, validation_loss : 0.5505,  0:00:00 &lt; 0:00:00                    epoch : 0068/0080, batch : 003, train_loss : 0.5563, validation_loss : 0.5505,  0:00:00 &lt; 0:00:00                    epoch : 0068/0080, batch : 004, train_loss : 0.4889, validation_loss : 0.5505,  0:00:00 &lt; 0:00:00                    epoch : 0069/0080, batch : 001, train_loss : 0.5787, validation_loss : 0.5452,  0:00:00 &lt; 0:00:00                    epoch : 0069/0080, batch : 002, train_loss : 0.5222, validation_loss : 0.5452,  0:00:00 &lt; 0:00:00                    epoch : 0069/0080, batch : 003, train_loss : 0.5871, validation_loss : 0.5452,  0:00:00 &lt; 0:00:00                    epoch : 0069/0080, batch : 004, train_loss : 0.4953, validation_loss : 0.5452,  0:00:00 &lt; 0:00:00                    epoch : 0070/0080, batch : 001, train_loss : 0.5285, validation_loss : 0.5402,  0:00:00 &lt; 0:00:00                    epoch : 0070/0080, batch : 002, train_loss : 0.5785, validation_loss : 0.5402,  0:00:00 &lt; 0:00:00                    epoch : 0070/0080, batch : 003, train_loss : 0.5232, validation_loss : 0.5402,  0:00:00 &lt; 0:00:00                    epoch : 0070/0080, batch : 004, train_loss : 0.5416, validation_loss : 0.5402,  0:00:00 &lt; 0:00:00                    epoch : 0071/0080, batch : 001, train_loss : 0.5268, validation_loss : 0.5334,  0:00:00 &lt; 0:00:00                    epoch : 0071/0080, batch : 002, train_loss : 0.5660, validation_loss : 0.5334,  0:00:00 &lt; 0:00:00                    epoch : 0071/0080, batch : 003, train_loss : 0.5138, validation_loss : 0.5334,  0:00:00 &lt; 0:00:00                    epoch : 0071/0080, batch : 004, train_loss : 0.5436, validation_loss : 0.5334,  0:00:00 &lt; 0:00:00                    epoch : 0072/0080, batch : 001, train_loss : 0.5295, validation_loss : 0.5273,  0:00:00 &lt; 0:00:00                    epoch : 0072/0080, batch : 002, train_loss : 0.4790, validation_loss : 0.5273,  0:00:00 &lt; 0:00:00                    epoch : 0072/0080, batch : 003, train_loss : 0.5329, validation_loss : 0.5273,  0:00:00 &lt; 0:00:00                    epoch : 0072/0080, batch : 004, train_loss : 0.6011, validation_loss : 0.5273,  0:00:00 &lt; 0:00:00                    epoch : 0073/0080, batch : 001, train_loss : 0.5067, validation_loss : 0.5225,  0:00:00 &lt; 0:00:00                    epoch : 0073/0080, batch : 002, train_loss : 0.5633, validation_loss : 0.5225,  0:00:00 &lt; 0:00:00                    epoch : 0073/0080, batch : 003, train_loss : 0.5292, validation_loss : 0.5225,  0:00:00 &lt; 0:00:00                    epoch : 0073/0080, batch : 004, train_loss : 0.4951, validation_loss : 0.5225,  0:00:00 &lt; 0:00:00                    epoch : 0074/0080, batch : 001, train_loss : 0.4815, validation_loss : 0.5171,  0:00:00 &lt; 0:00:00                    epoch : 0074/0080, batch : 002, train_loss : 0.5467, validation_loss : 0.5171,  0:00:00 &lt; 0:00:00                    epoch : 0074/0080, batch : 003, train_loss : 0.5224, validation_loss : 0.5171,  0:00:00 &lt; 0:00:00                    epoch : 0074/0080, batch : 004, train_loss : 0.5324, validation_loss : 0.5171,  0:00:00 &lt; 0:00:00                    epoch : 0075/0080, batch : 001, train_loss : 0.5352, validation_loss : 0.5122,  0:00:00 &lt; 0:00:00                    epoch : 0075/0080, batch : 002, train_loss : 0.5694, validation_loss : 0.5122,  0:00:00 &lt; 0:00:00                    epoch : 0075/0080, batch : 003, train_loss : 0.4827, validation_loss : 0.5122,  0:00:00 &lt; 0:00:00                    epoch : 0075/0080, batch : 004, train_loss : 0.4587, validation_loss : 0.5122,  0:00:00 &lt; 0:00:00                    epoch : 0076/0080, batch : 001, train_loss : 0.5194, validation_loss : 0.5083,  0:00:00 &lt; 0:00:00                    epoch : 0076/0080, batch : 002, train_loss : 0.5323, validation_loss : 0.5083,  0:00:00 &lt; 0:00:00                    epoch : 0076/0080, batch : 003, train_loss : 0.4281, validation_loss : 0.5083,  0:00:00 &lt; 0:00:00                    epoch : 0076/0080, batch : 004, train_loss : 0.5745, validation_loss : 0.5083,  0:00:00 &lt; 0:00:00                    epoch : 0077/0080, batch : 001, train_loss : 0.5869, validation_loss : 0.5026,  0:00:00 &lt; 0:00:00                    epoch : 0077/0080, batch : 002, train_loss : 0.4655, validation_loss : 0.5026,  0:00:00 &lt; 0:00:00                    epoch : 0077/0080, batch : 003, train_loss : 0.4567, validation_loss : 0.5026,  0:00:00 &lt; 0:00:00                    epoch : 0077/0080, batch : 004, train_loss : 0.5085, validation_loss : 0.5026,  0:00:00 &lt; 0:00:00                    epoch : 0078/0080, batch : 001, train_loss : 0.4860, validation_loss : 0.4959,  0:00:00 &lt; 0:00:00                    epoch : 0078/0080, batch : 002, train_loss : 0.5455, validation_loss : 0.4959,  0:00:00 &lt; 0:00:00                    epoch : 0078/0080, batch : 003, train_loss : 0.4778, validation_loss : 0.4959,  0:00:00 &lt; 0:00:00                    epoch : 0078/0080, batch : 004, train_loss : 0.4799, validation_loss : 0.4959,  0:00:00 &lt; 0:00:00                    epoch : 0079/0080, batch : 001, train_loss : 0.4958, validation_loss : 0.4896,  0:00:00 &lt; 0:00:00                    epoch : 0079/0080, batch : 002, train_loss : 0.5399, validation_loss : 0.4896,  0:00:00 &lt; 0:00:00                    epoch : 0079/0080, batch : 003, train_loss : 0.4709, validation_loss : 0.4896,  0:00:00 &lt; 0:00:00                    epoch : 0079/0080, batch : 004, train_loss : 0.4587, validation_loss : 0.4896,  0:00:00 &lt; 0:00:00                    epoch : 0080/0080, batch : 001, train_loss : 0.5086, validation_loss : 0.4835,  0:00:00 &lt; 0:00:00                    epoch : 0080/0080, batch : 002, train_loss : 0.5147, validation_loss : 0.4835,  0:00:00 &lt; 0:00:00                    epoch : 0080/0080, batch : 003, train_loss : 0.4495, validation_loss : 0.4835,  0:00:00 &lt; 0:00:00                    epoch : 0080/0080, batch : 004, train_loss : 0.4796, validation_loss : 0.4835,  0:00:00 &lt; 0:00:00                    ...training complete !!
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_mlutils.py::test_torchModel_with_loader</td>
          <td class="col-duration">0.23</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>Cuda is available and will be used for training
Training will be done on these cuda devices: [0]
epoch : 0001/0080, batch : 001, train_loss : 1.0988, validation_loss : nan,  0:00:00 &lt; 0:00:00                    epoch : 0001/0080, batch : 002, train_loss : 1.1085, validation_loss : nan,  0:00:00 &lt; 0:00:00                    epoch : 0001/0080, batch : 003, train_loss : 1.0961, validation_loss : nan,  0:00:00 &lt; 0:00:00                    epoch : 0001/0080, batch : 004, train_loss : 1.0830, validation_loss : nan,  0:00:00 &lt; 0:00:00                    epoch : 0002/0080, batch : 001, train_loss : 1.0778, validation_loss : 1.0907,  0:00:00 &lt; 0:00:00                    epoch : 0002/0080, batch : 002, train_loss : 1.0838, validation_loss : 1.0907,  0:00:00 &lt; 0:00:00                    epoch : 0002/0080, batch : 003, train_loss : 1.0741, validation_loss : 1.0907,  0:00:00 &lt; 0:00:00                    epoch : 0002/0080, batch : 004, train_loss : 1.0639, validation_loss : 1.0907,  0:00:00 &lt; 0:00:00                    epoch : 0003/0080, batch : 001, train_loss : 1.0598, validation_loss : 1.0640,  0:00:00 &lt; 0:00:00                    epoch : 0003/0080, batch : 002, train_loss : 1.0623, validation_loss : 1.0640,  0:00:00 &lt; 0:00:00                    epoch : 0003/0080, batch : 003, train_loss : 1.0551, validation_loss : 1.0640,  0:00:00 &lt; 0:00:00                    epoch : 0003/0080, batch : 004, train_loss : 1.0473, validation_loss : 1.0640,  0:00:00 &lt; 0:00:00                    epoch : 0004/0080, batch : 001, train_loss : 1.0443, validation_loss : 1.0412,  0:00:00 &lt; 0:00:00                    epoch : 0004/0080, batch : 002, train_loss : 1.0438, validation_loss : 1.0412,  0:00:00 &lt; 0:00:00                    epoch : 0004/0080, batch : 003, train_loss : 1.0389, validation_loss : 1.0412,  0:00:00 &lt; 0:00:00                    epoch : 0004/0080, batch : 004, train_loss : 1.0338, validation_loss : 1.0412,  0:00:00 &lt; 0:00:00                    epoch : 0005/0080, batch : 001, train_loss : 1.0315, validation_loss : 1.0225,  0:00:00 &lt; 0:00:00                    epoch : 0005/0080, batch : 002, train_loss : 1.0286, validation_loss : 1.0225,  0:00:00 &lt; 0:00:00                    epoch : 0005/0080, batch : 003, train_loss : 1.0253, validation_loss : 1.0225,  0:00:00 &lt; 0:00:00                    epoch : 0005/0080, batch : 004, train_loss : 1.0220, validation_loss : 1.0225,  0:00:00 &lt; 0:00:00                    epoch : 0006/0080, batch : 001, train_loss : 1.0205, validation_loss : 1.0068,  0:00:00 &lt; 0:00:00                    epoch : 0006/0080, batch : 002, train_loss : 1.0156, validation_loss : 1.0068,  0:00:00 &lt; 0:00:00                    epoch : 0006/0080, batch : 003, train_loss : 1.0133, validation_loss : 1.0068,  0:00:00 &lt; 0:00:00                    epoch : 0006/0080, batch : 004, train_loss : 1.0115, validation_loss : 1.0068,  0:00:00 &lt; 0:00:00                    epoch : 0007/0080, batch : 001, train_loss : 1.0102, validation_loss : 0.9935,  0:00:00 &lt; 0:00:00                    epoch : 0007/0080, batch : 002, train_loss : 1.0040, validation_loss : 0.9935,  0:00:00 &lt; 0:00:00                    epoch : 0007/0080, batch : 003, train_loss : 1.0020, validation_loss : 0.9935,  0:00:00 &lt; 0:00:00                    epoch : 0007/0080, batch : 004, train_loss : 1.0015, validation_loss : 0.9935,  0:00:00 &lt; 0:00:00                    epoch : 0008/0080, batch : 001, train_loss : 1.0002, validation_loss : 0.9815,  0:00:00 &lt; 0:00:00                    epoch : 0008/0080, batch : 002, train_loss : 0.9932, validation_loss : 0.9815,  0:00:00 &lt; 0:00:00                    epoch : 0008/0080, batch : 003, train_loss : 0.9910, validation_loss : 0.9815,  0:00:00 &lt; 0:00:00                    epoch : 0008/0080, batch : 004, train_loss : 0.9920, validation_loss : 0.9815,  0:00:00 &lt; 0:00:00                    epoch : 0009/0080, batch : 001, train_loss : 0.9904, validation_loss : 0.9702,  0:00:00 &lt; 0:00:00                    epoch : 0009/0080, batch : 002, train_loss : 0.9830, validation_loss : 0.9702,  0:00:00 &lt; 0:00:00                    epoch : 0009/0080, batch : 003, train_loss : 0.9803, validation_loss : 0.9702,  0:00:00 &lt; 0:00:00                    epoch : 0009/0080, batch : 004, train_loss : 0.9827, validation_loss : 0.9702,  0:00:00 &lt; 0:00:00                    epoch : 0010/0080, batch : 001, train_loss : 0.9804, validation_loss : 0.9596,  0:00:00 &lt; 0:00:00                    epoch : 0010/0080, batch : 002, train_loss : 0.9731, validation_loss : 0.9596,  0:00:00 &lt; 0:00:00                    epoch : 0010/0080, batch : 003, train_loss : 0.9697, validation_loss : 0.9596,  0:00:00 &lt; 0:00:00                    epoch : 0010/0080, batch : 004, train_loss : 0.9736, validation_loss : 0.9596,  0:00:00 &lt; 0:00:00                    epoch : 0011/0080, batch : 001, train_loss : 0.9706, validation_loss : 0.9494,  0:00:00 &lt; 0:00:00                    epoch : 0011/0080, batch : 002, train_loss : 0.9633, validation_loss : 0.9494,  0:00:00 &lt; 0:00:00                    epoch : 0011/0080, batch : 003, train_loss : 0.9592, validation_loss : 0.9494,  0:00:00 &lt; 0:00:00                    epoch : 0011/0080, batch : 004, train_loss : 0.9648, validation_loss : 0.9494,  0:00:00 &lt; 0:00:00                    epoch : 0012/0080, batch : 001, train_loss : 0.9607, validation_loss : 0.9399,  0:00:00 &lt; 0:00:00                    epoch : 0012/0080, batch : 002, train_loss : 0.9541, validation_loss : 0.9399,  0:00:00 &lt; 0:00:00                    epoch : 0012/0080, batch : 003, train_loss : 0.9491, validation_loss : 0.9399,  0:00:00 &lt; 0:00:00                    epoch : 0012/0080, batch : 004, train_loss : 0.9566, validation_loss : 0.9399,  0:00:00 &lt; 0:00:00                    epoch : 0013/0080, batch : 001, train_loss : 0.9513, validation_loss : 0.9310,  0:00:00 &lt; 0:00:00                    epoch : 0013/0080, batch : 002, train_loss : 0.9452, validation_loss : 0.9310,  0:00:00 &lt; 0:00:00                    epoch : 0013/0080, batch : 003, train_loss : 0.9395, validation_loss : 0.9310,  0:00:00 &lt; 0:00:00                    epoch : 0013/0080, batch : 004, train_loss : 0.9485, validation_loss : 0.9310,  0:00:00 &lt; 0:00:00                    epoch : 0014/0080, batch : 001, train_loss : 0.9420, validation_loss : 0.9226,  0:00:00 &lt; 0:00:00                    epoch : 0014/0080, batch : 002, train_loss : 0.9364, validation_loss : 0.9226,  0:00:00 &lt; 0:00:00                    epoch : 0014/0080, batch : 003, train_loss : 0.9299, validation_loss : 0.9226,  0:00:00 &lt; 0:00:00                    epoch : 0014/0080, batch : 004, train_loss : 0.9405, validation_loss : 0.9226,  0:00:00 &lt; 0:00:00                    epoch : 0015/0080, batch : 001, train_loss : 0.9328, validation_loss : 0.9140,  0:00:00 &lt; 0:00:00                    epoch : 0015/0080, batch : 002, train_loss : 0.9275, validation_loss : 0.9140,  0:00:00 &lt; 0:00:00                    epoch : 0015/0080, batch : 003, train_loss : 0.9203, validation_loss : 0.9140,  0:00:00 &lt; 0:00:00                    epoch : 0015/0080, batch : 004, train_loss : 0.9326, validation_loss : 0.9140,  0:00:00 &lt; 0:00:00                    epoch : 0016/0080, batch : 001, train_loss : 0.9236, validation_loss : 0.9054,  0:00:00 &lt; 0:00:00                    epoch : 0016/0080, batch : 002, train_loss : 0.9186, validation_loss : 0.9054,  0:00:00 &lt; 0:00:00                    epoch : 0016/0080, batch : 003, train_loss : 0.9107, validation_loss : 0.9054,  0:00:00 &lt; 0:00:00                    epoch : 0016/0080, batch : 004, train_loss : 0.9248, validation_loss : 0.9054,  0:00:00 &lt; 0:00:00                    epoch : 0017/0080, batch : 001, train_loss : 0.9146, validation_loss : 0.8966,  0:00:00 &lt; 0:00:00                    epoch : 0017/0080, batch : 002, train_loss : 0.9097, validation_loss : 0.8966,  0:00:00 &lt; 0:00:00                    epoch : 0017/0080, batch : 003, train_loss : 0.9010, validation_loss : 0.8966,  0:00:00 &lt; 0:00:00                    epoch : 0017/0080, batch : 004, train_loss : 0.9172, validation_loss : 0.8966,  0:00:00 &lt; 0:00:00                    epoch : 0018/0080, batch : 001, train_loss : 0.9058, validation_loss : 0.8877,  0:00:00 &lt; 0:00:00                    epoch : 0018/0080, batch : 002, train_loss : 0.9008, validation_loss : 0.8877,  0:00:00 &lt; 0:00:00                    epoch : 0018/0080, batch : 003, train_loss : 0.8913, validation_loss : 0.8877,  0:00:00 &lt; 0:00:00                    epoch : 0018/0080, batch : 004, train_loss : 0.9095, validation_loss : 0.8877,  0:00:00 &lt; 0:00:00                    epoch : 0019/0080, batch : 001, train_loss : 0.8969, validation_loss : 0.8788,  0:00:00 &lt; 0:00:00                    epoch : 0019/0080, batch : 002, train_loss : 0.8919, validation_loss : 0.8788,  0:00:00 &lt; 0:00:00                    epoch : 0019/0080, batch : 003, train_loss : 0.8816, validation_loss : 0.8788,  0:00:00 &lt; 0:00:00                    epoch : 0019/0080, batch : 004, train_loss : 0.9019, validation_loss : 0.8788,  0:00:00 &lt; 0:00:00                    epoch : 0020/0080, batch : 001, train_loss : 0.8880, validation_loss : 0.8699,  0:00:00 &lt; 0:00:00                    epoch : 0020/0080, batch : 002, train_loss : 0.8829, validation_loss : 0.8699,  0:00:00 &lt; 0:00:00                    epoch : 0020/0080, batch : 003, train_loss : 0.8720, validation_loss : 0.8699,  0:00:00 &lt; 0:00:00                    epoch : 0020/0080, batch : 004, train_loss : 0.8942, validation_loss : 0.8699,  0:00:00 &lt; 0:00:00                    epoch : 0021/0080, batch : 001, train_loss : 0.8791, validation_loss : 0.8610,  0:00:00 &lt; 0:00:00                    epoch : 0021/0080, batch : 002, train_loss : 0.8739, validation_loss : 0.8610,  0:00:00 &lt; 0:00:00                    epoch : 0021/0080, batch : 003, train_loss : 0.8623, validation_loss : 0.8610,  0:00:00 &lt; 0:00:00                    epoch : 0021/0080, batch : 004, train_loss : 0.8864, validation_loss : 0.8610,  0:00:00 &lt; 0:00:00                    epoch : 0022/0080, batch : 001, train_loss : 0.8701, validation_loss : 0.8521,  0:00:00 &lt; 0:00:00                    epoch : 0022/0080, batch : 002, train_loss : 0.8649, validation_loss : 0.8521,  0:00:00 &lt; 0:00:00                    epoch : 0022/0080, batch : 003, train_loss : 0.8527, validation_loss : 0.8521,  0:00:00 &lt; 0:00:00                    epoch : 0022/0080, batch : 004, train_loss : 0.8785, validation_loss : 0.8521,  0:00:00 &lt; 0:00:00                    epoch : 0023/0080, batch : 001, train_loss : 0.8611, validation_loss : 0.8432,  0:00:00 &lt; 0:00:00                    epoch : 0023/0080, batch : 002, train_loss : 0.8559, validation_loss : 0.8432,  0:00:00 &lt; 0:00:00                    epoch : 0023/0080, batch : 003, train_loss : 0.8431, validation_loss : 0.8432,  0:00:00 &lt; 0:00:00                    epoch : 0023/0080, batch : 004, train_loss : 0.8705, validation_loss : 0.8432,  0:00:00 &lt; 0:00:00                    epoch : 0024/0080, batch : 001, train_loss : 0.8521, validation_loss : 0.8343,  0:00:00 &lt; 0:00:00                    epoch : 0024/0080, batch : 002, train_loss : 0.8469, validation_loss : 0.8343,  0:00:00 &lt; 0:00:00                    epoch : 0024/0080, batch : 003, train_loss : 0.8335, validation_loss : 0.8343,  0:00:00 &lt; 0:00:00                    epoch : 0024/0080, batch : 004, train_loss : 0.8626, validation_loss : 0.8343,  0:00:00 &lt; 0:00:00                    epoch : 0025/0080, batch : 001, train_loss : 0.8430, validation_loss : 0.8254,  0:00:00 &lt; 0:00:00                    epoch : 0025/0080, batch : 002, train_loss : 0.8378, validation_loss : 0.8254,  0:00:00 &lt; 0:00:00                    epoch : 0025/0080, batch : 003, train_loss : 0.8239, validation_loss : 0.8254,  0:00:00 &lt; 0:00:00                    epoch : 0025/0080, batch : 004, train_loss : 0.8546, validation_loss : 0.8254,  0:00:00 &lt; 0:00:00                    epoch : 0026/0080, batch : 001, train_loss : 0.8340, validation_loss : 0.8164,  0:00:00 &lt; 0:00:00                    epoch : 0026/0080, batch : 002, train_loss : 0.8288, validation_loss : 0.8164,  0:00:00 &lt; 0:00:00                    epoch : 0026/0080, batch : 003, train_loss : 0.8143, validation_loss : 0.8164,  0:00:00 &lt; 0:00:00                    epoch : 0026/0080, batch : 004, train_loss : 0.8466, validation_loss : 0.8164,  0:00:00 &lt; 0:00:00                    epoch : 0027/0080, batch : 001, train_loss : 0.8249, validation_loss : 0.8074,  0:00:00 &lt; 0:00:00                    epoch : 0027/0080, batch : 002, train_loss : 0.8197, validation_loss : 0.8074,  0:00:00 &lt; 0:00:00                    epoch : 0027/0080, batch : 003, train_loss : 0.8047, validation_loss : 0.8074,  0:00:00 &lt; 0:00:00                    epoch : 0027/0080, batch : 004, train_loss : 0.8385, validation_loss : 0.8074,  0:00:00 &lt; 0:00:00                    epoch : 0028/0080, batch : 001, train_loss : 0.8159, validation_loss : 0.7984,  0:00:00 &lt; 0:00:00                    epoch : 0028/0080, batch : 002, train_loss : 0.8107, validation_loss : 0.7984,  0:00:00 &lt; 0:00:00                    epoch : 0028/0080, batch : 003, train_loss : 0.7951, validation_loss : 0.7984,  0:00:00 &lt; 0:00:00                    epoch : 0028/0080, batch : 004, train_loss : 0.8304, validation_loss : 0.7984,  0:00:00 &lt; 0:00:00                    epoch : 0029/0080, batch : 001, train_loss : 0.8068, validation_loss : 0.7895,  0:00:00 &lt; 0:00:00                    epoch : 0029/0080, batch : 002, train_loss : 0.8016, validation_loss : 0.7895,  0:00:00 &lt; 0:00:00                    epoch : 0029/0080, batch : 003, train_loss : 0.7855, validation_loss : 0.7895,  0:00:00 &lt; 0:00:00                    epoch : 0029/0080, batch : 004, train_loss : 0.8222, validation_loss : 0.7895,  0:00:00 &lt; 0:00:00                    epoch : 0030/0080, batch : 001, train_loss : 0.7978, validation_loss : 0.7805,  0:00:00 &lt; 0:00:00                    epoch : 0030/0080, batch : 002, train_loss : 0.7926, validation_loss : 0.7805,  0:00:00 &lt; 0:00:00                    epoch : 0030/0080, batch : 003, train_loss : 0.7759, validation_loss : 0.7805,  0:00:00 &lt; 0:00:00                    epoch : 0030/0080, batch : 004, train_loss : 0.8141, validation_loss : 0.7805,  0:00:00 &lt; 0:00:00                    epoch : 0031/0080, batch : 001, train_loss : 0.7887, validation_loss : 0.7716,  0:00:00 &lt; 0:00:00                    epoch : 0031/0080, batch : 002, train_loss : 0.7836, validation_loss : 0.7716,  0:00:00 &lt; 0:00:00                    epoch : 0031/0080, batch : 003, train_loss : 0.7663, validation_loss : 0.7716,  0:00:00 &lt; 0:00:00                    epoch : 0031/0080, batch : 004, train_loss : 0.8060, validation_loss : 0.7716,  0:00:00 &lt; 0:00:00                    epoch : 0032/0080, batch : 001, train_loss : 0.7797, validation_loss : 0.7627,  0:00:00 &lt; 0:00:00                    epoch : 0032/0080, batch : 002, train_loss : 0.7746, validation_loss : 0.7627,  0:00:00 &lt; 0:00:00                    epoch : 0032/0080, batch : 003, train_loss : 0.7567, validation_loss : 0.7627,  0:00:00 &lt; 0:00:00                    epoch : 0032/0080, batch : 004, train_loss : 0.7978, validation_loss : 0.7627,  0:00:00 &lt; 0:00:00                    epoch : 0033/0080, batch : 001, train_loss : 0.7706, validation_loss : 0.7537,  0:00:00 &lt; 0:00:00                    epoch : 0033/0080, batch : 002, train_loss : 0.7656, validation_loss : 0.7537,  0:00:00 &lt; 0:00:00                    epoch : 0033/0080, batch : 003, train_loss : 0.7472, validation_loss : 0.7537,  0:00:00 &lt; 0:00:00                    epoch : 0033/0080, batch : 004, train_loss : 0.7897, validation_loss : 0.7537,  0:00:00 &lt; 0:00:00                    epoch : 0034/0080, batch : 001, train_loss : 0.7616, validation_loss : 0.7448,  0:00:00 &lt; 0:00:00                    epoch : 0034/0080, batch : 002, train_loss : 0.7566, validation_loss : 0.7448,  0:00:00 &lt; 0:00:00                    epoch : 0034/0080, batch : 003, train_loss : 0.7376, validation_loss : 0.7448,  0:00:00 &lt; 0:00:00                    epoch : 0034/0080, batch : 004, train_loss : 0.7816, validation_loss : 0.7448,  0:00:00 &lt; 0:00:00                    epoch : 0035/0080, batch : 001, train_loss : 0.7526, validation_loss : 0.7359,  0:00:00 &lt; 0:00:00                    epoch : 0035/0080, batch : 002, train_loss : 0.7476, validation_loss : 0.7359,  0:00:00 &lt; 0:00:00                    epoch : 0035/0080, batch : 003, train_loss : 0.7281, validation_loss : 0.7359,  0:00:00 &lt; 0:00:00                    epoch : 0035/0080, batch : 004, train_loss : 0.7735, validation_loss : 0.7359,  0:00:00 &lt; 0:00:00                    epoch : 0036/0080, batch : 001, train_loss : 0.7436, validation_loss : 0.7270,  0:00:00 &lt; 0:00:00                    epoch : 0036/0080, batch : 002, train_loss : 0.7387, validation_loss : 0.7270,  0:00:00 &lt; 0:00:00                    epoch : 0036/0080, batch : 003, train_loss : 0.7187, validation_loss : 0.7270,  0:00:00 &lt; 0:00:00                    epoch : 0036/0080, batch : 004, train_loss : 0.7655, validation_loss : 0.7270,  0:00:00 &lt; 0:00:00                    epoch : 0037/0080, batch : 001, train_loss : 0.7346, validation_loss : 0.7181,  0:00:00 &lt; 0:00:00                    epoch : 0037/0080, batch : 002, train_loss : 0.7298, validation_loss : 0.7181,  0:00:00 &lt; 0:00:00                    epoch : 0037/0080, batch : 003, train_loss : 0.7093, validation_loss : 0.7181,  0:00:00 &lt; 0:00:00                    epoch : 0037/0080, batch : 004, train_loss : 0.7575, validation_loss : 0.7181,  0:00:00 &lt; 0:00:00                    epoch : 0038/0080, batch : 001, train_loss : 0.7256, validation_loss : 0.7094,  0:00:00 &lt; 0:00:00                    epoch : 0038/0080, batch : 002, train_loss : 0.7209, validation_loss : 0.7094,  0:00:00 &lt; 0:00:00                    epoch : 0038/0080, batch : 003, train_loss : 0.7000, validation_loss : 0.7094,  0:00:00 &lt; 0:00:00                    epoch : 0038/0080, batch : 004, train_loss : 0.7494, validation_loss : 0.7094,  0:00:00 &lt; 0:00:00                    epoch : 0039/0080, batch : 001, train_loss : 0.7167, validation_loss : 0.7007,  0:00:00 &lt; 0:00:00                    epoch : 0039/0080, batch : 002, train_loss : 0.7122, validation_loss : 0.7007,  0:00:00 &lt; 0:00:00                    epoch : 0039/0080, batch : 003, train_loss : 0.6907, validation_loss : 0.7007,  0:00:00 &lt; 0:00:00                    epoch : 0039/0080, batch : 004, train_loss : 0.7414, validation_loss : 0.7007,  0:00:00 &lt; 0:00:00                    epoch : 0040/0080, batch : 001, train_loss : 0.7078, validation_loss : 0.6921,  0:00:00 &lt; 0:00:00                    epoch : 0040/0080, batch : 002, train_loss : 0.7034, validation_loss : 0.6921,  0:00:00 &lt; 0:00:00                    epoch : 0040/0080, batch : 003, train_loss : 0.6815, validation_loss : 0.6921,  0:00:00 &lt; 0:00:00                    epoch : 0040/0080, batch : 004, train_loss : 0.7334, validation_loss : 0.6921,  0:00:00 &lt; 0:00:00                    epoch : 0041/0080, batch : 001, train_loss : 0.6990, validation_loss : 0.6836,  0:00:00 &lt; 0:00:00                    epoch : 0041/0080, batch : 002, train_loss : 0.6948, validation_loss : 0.6836,  0:00:00 &lt; 0:00:00                    epoch : 0041/0080, batch : 003, train_loss : 0.6724, validation_loss : 0.6836,  0:00:00 &lt; 0:00:00                    epoch : 0041/0080, batch : 004, train_loss : 0.7254, validation_loss : 0.6836,  0:00:00 &lt; 0:00:00                    epoch : 0042/0080, batch : 001, train_loss : 0.6902, validation_loss : 0.6752,  0:00:00 &lt; 0:00:00                    epoch : 0042/0080, batch : 002, train_loss : 0.6862, validation_loss : 0.6752,  0:00:00 &lt; 0:00:00                    epoch : 0042/0080, batch : 003, train_loss : 0.6634, validation_loss : 0.6752,  0:00:00 &lt; 0:00:00                    epoch : 0042/0080, batch : 004, train_loss : 0.7174, validation_loss : 0.6752,  0:00:00 &lt; 0:00:00                    epoch : 0043/0080, batch : 001, train_loss : 0.6815, validation_loss : 0.6667,  0:00:00 &lt; 0:00:00                    epoch : 0043/0080, batch : 002, train_loss : 0.6776, validation_loss : 0.6667,  0:00:00 &lt; 0:00:00                    epoch : 0043/0080, batch : 003, train_loss : 0.6545, validation_loss : 0.6667,  0:00:00 &lt; 0:00:00                    epoch : 0043/0080, batch : 004, train_loss : 0.7095, validation_loss : 0.6667,  0:00:00 &lt; 0:00:00                    epoch : 0044/0080, batch : 001, train_loss : 0.6729, validation_loss : 0.6584,  0:00:00 &lt; 0:00:00                    epoch : 0044/0080, batch : 002, train_loss : 0.6691, validation_loss : 0.6584,  0:00:00 &lt; 0:00:00                    epoch : 0044/0080, batch : 003, train_loss : 0.6456, validation_loss : 0.6584,  0:00:00 &lt; 0:00:00                    epoch : 0044/0080, batch : 004, train_loss : 0.7015, validation_loss : 0.6584,  0:00:00 &lt; 0:00:00                    epoch : 0045/0080, batch : 001, train_loss : 0.6643, validation_loss : 0.6501,  0:00:00 &lt; 0:00:00                    epoch : 0045/0080, batch : 002, train_loss : 0.6607, validation_loss : 0.6501,  0:00:00 &lt; 0:00:00                    epoch : 0045/0080, batch : 003, train_loss : 0.6369, validation_loss : 0.6501,  0:00:00 &lt; 0:00:00                    epoch : 0045/0080, batch : 004, train_loss : 0.6936, validation_loss : 0.6501,  0:00:00 &lt; 0:00:00                    epoch : 0046/0080, batch : 001, train_loss : 0.6558, validation_loss : 0.6419,  0:00:00 &lt; 0:00:00                    epoch : 0046/0080, batch : 002, train_loss : 0.6524, validation_loss : 0.6419,  0:00:00 &lt; 0:00:00                    epoch : 0046/0080, batch : 003, train_loss : 0.6282, validation_loss : 0.6419,  0:00:00 &lt; 0:00:00                    epoch : 0046/0080, batch : 004, train_loss : 0.6857, validation_loss : 0.6419,  0:00:00 &lt; 0:00:00                    epoch : 0047/0080, batch : 001, train_loss : 0.6473, validation_loss : 0.6337,  0:00:00 &lt; 0:00:00                    epoch : 0047/0080, batch : 002, train_loss : 0.6441, validation_loss : 0.6337,  0:00:00 &lt; 0:00:00                    epoch : 0047/0080, batch : 003, train_loss : 0.6196, validation_loss : 0.6337,  0:00:00 &lt; 0:00:00                    epoch : 0047/0080, batch : 004, train_loss : 0.6779, validation_loss : 0.6337,  0:00:00 &lt; 0:00:00                    epoch : 0048/0080, batch : 001, train_loss : 0.6389, validation_loss : 0.6257,  0:00:00 &lt; 0:00:00                    epoch : 0048/0080, batch : 002, train_loss : 0.6359, validation_loss : 0.6257,  0:00:00 &lt; 0:00:00                    epoch : 0048/0080, batch : 003, train_loss : 0.6112, validation_loss : 0.6257,  0:00:00 &lt; 0:00:00                    epoch : 0048/0080, batch : 004, train_loss : 0.6701, validation_loss : 0.6257,  0:00:00 &lt; 0:00:00                    epoch : 0049/0080, batch : 001, train_loss : 0.6306, validation_loss : 0.6178,  0:00:00 &lt; 0:00:00                    epoch : 0049/0080, batch : 002, train_loss : 0.6278, validation_loss : 0.6178,  0:00:00 &lt; 0:00:00                    epoch : 0049/0080, batch : 003, train_loss : 0.6028, validation_loss : 0.6178,  0:00:00 &lt; 0:00:00                    epoch : 0049/0080, batch : 004, train_loss : 0.6623, validation_loss : 0.6178,  0:00:00 &lt; 0:00:00                    epoch : 0050/0080, batch : 001, train_loss : 0.6224, validation_loss : 0.6100,  0:00:00 &lt; 0:00:00                    epoch : 0050/0080, batch : 002, train_loss : 0.6198, validation_loss : 0.6100,  0:00:00 &lt; 0:00:00                    epoch : 0050/0080, batch : 003, train_loss : 0.5946, validation_loss : 0.6100,  0:00:00 &lt; 0:00:00                    epoch : 0050/0080, batch : 004, train_loss : 0.6546, validation_loss : 0.6100,  0:00:00 &lt; 0:00:00                    epoch : 0051/0080, batch : 001, train_loss : 0.6142, validation_loss : 0.6023,  0:00:00 &lt; 0:00:00                    epoch : 0051/0080, batch : 002, train_loss : 0.6118, validation_loss : 0.6023,  0:00:00 &lt; 0:00:00                    epoch : 0051/0080, batch : 003, train_loss : 0.5864, validation_loss : 0.6023,  0:00:00 &lt; 0:00:00                    epoch : 0051/0080, batch : 004, train_loss : 0.6470, validation_loss : 0.6023,  0:00:00 &lt; 0:00:00                    epoch : 0052/0080, batch : 001, train_loss : 0.6062, validation_loss : 0.5947,  0:00:00 &lt; 0:00:00                    epoch : 0052/0080, batch : 002, train_loss : 0.6040, validation_loss : 0.5947,  0:00:00 &lt; 0:00:00                    epoch : 0052/0080, batch : 003, train_loss : 0.5784, validation_loss : 0.5947,  0:00:00 &lt; 0:00:00                    epoch : 0052/0080, batch : 004, train_loss : 0.6394, validation_loss : 0.5947,  0:00:00 &lt; 0:00:00                    epoch : 0053/0080, batch : 001, train_loss : 0.5983, validation_loss : 0.5871,  0:00:00 &lt; 0:00:00                    epoch : 0053/0080, batch : 002, train_loss : 0.5963, validation_loss : 0.5871,  0:00:00 &lt; 0:00:00                    epoch : 0053/0080, batch : 003, train_loss : 0.5705, validation_loss : 0.5871,  0:00:00 &lt; 0:00:00                    epoch : 0053/0080, batch : 004, train_loss : 0.6319, validation_loss : 0.5871,  0:00:00 &lt; 0:00:00                    epoch : 0054/0080, batch : 001, train_loss : 0.5906, validation_loss : 0.5796,  0:00:00 &lt; 0:00:00                    epoch : 0054/0080, batch : 002, train_loss : 0.5886, validation_loss : 0.5796,  0:00:00 &lt; 0:00:00                    epoch : 0054/0080, batch : 003, train_loss : 0.5627, validation_loss : 0.5796,  0:00:00 &lt; 0:00:00                    epoch : 0054/0080, batch : 004, train_loss : 0.6245, validation_loss : 0.5796,  0:00:00 &lt; 0:00:00                    epoch : 0055/0080, batch : 001, train_loss : 0.5829, validation_loss : 0.5722,  0:00:00 &lt; 0:00:00                    epoch : 0055/0080, batch : 002, train_loss : 0.5810, validation_loss : 0.5722,  0:00:00 &lt; 0:00:00                    epoch : 0055/0080, batch : 003, train_loss : 0.5550, validation_loss : 0.5722,  0:00:00 &lt; 0:00:00                    epoch : 0055/0080, batch : 004, train_loss : 0.6172, validation_loss : 0.5722,  0:00:00 &lt; 0:00:00                    epoch : 0056/0080, batch : 001, train_loss : 0.5753, validation_loss : 0.5649,  0:00:00 &lt; 0:00:00                    epoch : 0056/0080, batch : 002, train_loss : 0.5736, validation_loss : 0.5649,  0:00:00 &lt; 0:00:00                    epoch : 0056/0080, batch : 003, train_loss : 0.5474, validation_loss : 0.5649,  0:00:00 &lt; 0:00:00                    epoch : 0056/0080, batch : 004, train_loss : 0.6099, validation_loss : 0.5649,  0:00:00 &lt; 0:00:00                    epoch : 0057/0080, batch : 001, train_loss : 0.5678, validation_loss : 0.5577,  0:00:00 &lt; 0:00:00                    epoch : 0057/0080, batch : 002, train_loss : 0.5662, validation_loss : 0.5577,  0:00:00 &lt; 0:00:00                    epoch : 0057/0080, batch : 003, train_loss : 0.5400, validation_loss : 0.5577,  0:00:00 &lt; 0:00:00                    epoch : 0057/0080, batch : 004, train_loss : 0.6027, validation_loss : 0.5577,  0:00:00 &lt; 0:00:00                    epoch : 0058/0080, batch : 001, train_loss : 0.5604, validation_loss : 0.5505,  0:00:00 &lt; 0:00:00                    epoch : 0058/0080, batch : 002, train_loss : 0.5590, validation_loss : 0.5505,  0:00:00 &lt; 0:00:00                    epoch : 0058/0080, batch : 003, train_loss : 0.5326, validation_loss : 0.5505,  0:00:00 &lt; 0:00:00                    epoch : 0058/0080, batch : 004, train_loss : 0.5955, validation_loss : 0.5505,  0:00:00 &lt; 0:00:00                    epoch : 0059/0080, batch : 001, train_loss : 0.5531, validation_loss : 0.5434,  0:00:00 &lt; 0:00:00                    epoch : 0059/0080, batch : 002, train_loss : 0.5518, validation_loss : 0.5434,  0:00:00 &lt; 0:00:00                    epoch : 0059/0080, batch : 003, train_loss : 0.5253, validation_loss : 0.5434,  0:00:00 &lt; 0:00:00                    epoch : 0059/0080, batch : 004, train_loss : 0.5884, validation_loss : 0.5434,  0:00:00 &lt; 0:00:00                    epoch : 0060/0080, batch : 001, train_loss : 0.5459, validation_loss : 0.5364,  0:00:00 &lt; 0:00:00                    epoch : 0060/0080, batch : 002, train_loss : 0.5447, validation_loss : 0.5364,  0:00:00 &lt; 0:00:00                    epoch : 0060/0080, batch : 003, train_loss : 0.5182, validation_loss : 0.5364,  0:00:00 &lt; 0:00:00                    epoch : 0060/0080, batch : 004, train_loss : 0.5814, validation_loss : 0.5364,  0:00:00 &lt; 0:00:00                    epoch : 0061/0080, batch : 001, train_loss : 0.5388, validation_loss : 0.5297,  0:00:00 &lt; 0:00:00                    epoch : 0061/0080, batch : 002, train_loss : 0.5378, validation_loss : 0.5297,  0:00:00 &lt; 0:00:00                    epoch : 0061/0080, batch : 003, train_loss : 0.5112, validation_loss : 0.5297,  0:00:00 &lt; 0:00:00                    epoch : 0061/0080, batch : 004, train_loss : 0.5744, validation_loss : 0.5297,  0:00:00 &lt; 0:00:00                    epoch : 0062/0080, batch : 001, train_loss : 0.5318, validation_loss : 0.5228,  0:00:00 &lt; 0:00:00                    epoch : 0062/0080, batch : 002, train_loss : 0.5311, validation_loss : 0.5228,  0:00:00 &lt; 0:00:00                    epoch : 0062/0080, batch : 003, train_loss : 0.5043, validation_loss : 0.5228,  0:00:00 &lt; 0:00:00                    epoch : 0062/0080, batch : 004, train_loss : 0.5676, validation_loss : 0.5228,  0:00:00 &lt; 0:00:00                    epoch : 0063/0080, batch : 001, train_loss : 0.5249, validation_loss : 0.5162,  0:00:00 &lt; 0:00:00                    epoch : 0063/0080, batch : 002, train_loss : 0.5245, validation_loss : 0.5162,  0:00:00 &lt; 0:00:00                    epoch : 0063/0080, batch : 003, train_loss : 0.4975, validation_loss : 0.5162,  0:00:00 &lt; 0:00:00                    epoch : 0063/0080, batch : 004, train_loss : 0.5608, validation_loss : 0.5162,  0:00:00 &lt; 0:00:00                    epoch : 0064/0080, batch : 001, train_loss : 0.5181, validation_loss : 0.5098,  0:00:00 &lt; 0:00:00                    epoch : 0064/0080, batch : 002, train_loss : 0.5180, validation_loss : 0.5098,  0:00:00 &lt; 0:00:00                    epoch : 0064/0080, batch : 003, train_loss : 0.4909, validation_loss : 0.5098,  0:00:00 &lt; 0:00:00                    epoch : 0064/0080, batch : 004, train_loss : 0.5541, validation_loss : 0.5098,  0:00:00 &lt; 0:00:00                    epoch : 0065/0080, batch : 001, train_loss : 0.5114, validation_loss : 0.5034,  0:00:00 &lt; 0:00:00                    epoch : 0065/0080, batch : 002, train_loss : 0.5117, validation_loss : 0.5034,  0:00:00 &lt; 0:00:00                    epoch : 0065/0080, batch : 003, train_loss : 0.4844, validation_loss : 0.5034,  0:00:00 &lt; 0:00:00                    epoch : 0065/0080, batch : 004, train_loss : 0.5475, validation_loss : 0.5034,  0:00:00 &lt; 0:00:00                    epoch : 0066/0080, batch : 001, train_loss : 0.5049, validation_loss : 0.4971,  0:00:00 &lt; 0:00:00                    epoch : 0066/0080, batch : 002, train_loss : 0.5055, validation_loss : 0.4971,  0:00:00 &lt; 0:00:00                    epoch : 0066/0080, batch : 003, train_loss : 0.4781, validation_loss : 0.4971,  0:00:00 &lt; 0:00:00                    epoch : 0066/0080, batch : 004, train_loss : 0.5411, validation_loss : 0.4971,  0:00:00 &lt; 0:00:00                    epoch : 0067/0080, batch : 001, train_loss : 0.4985, validation_loss : 0.4910,  0:00:00 &lt; 0:00:00                    epoch : 0067/0080, batch : 002, train_loss : 0.4994, validation_loss : 0.4910,  0:00:00 &lt; 0:00:00                    epoch : 0067/0080, batch : 003, train_loss : 0.4718, validation_loss : 0.4910,  0:00:00 &lt; 0:00:00                    epoch : 0067/0080, batch : 004, train_loss : 0.5348, validation_loss : 0.4910,  0:00:00 &lt; 0:00:00                    epoch : 0068/0080, batch : 001, train_loss : 0.4923, validation_loss : 0.4849,  0:00:00 &lt; 0:00:00                    epoch : 0068/0080, batch : 002, train_loss : 0.4935, validation_loss : 0.4849,  0:00:00 &lt; 0:00:00                    epoch : 0068/0080, batch : 003, train_loss : 0.4657, validation_loss : 0.4849,  0:00:00 &lt; 0:00:00                    epoch : 0068/0080, batch : 004, train_loss : 0.5286, validation_loss : 0.4849,  0:00:00 &lt; 0:00:00                    epoch : 0069/0080, batch : 001, train_loss : 0.4862, validation_loss : 0.4789,  0:00:00 &lt; 0:00:00                    epoch : 0069/0080, batch : 002, train_loss : 0.4877, validation_loss : 0.4789,  0:00:00 &lt; 0:00:00                    epoch : 0069/0080, batch : 003, train_loss : 0.4597, validation_loss : 0.4789,  0:00:00 &lt; 0:00:00                    epoch : 0069/0080, batch : 004, train_loss : 0.5225, validation_loss : 0.4789,  0:00:00 &lt; 0:00:00                    epoch : 0070/0080, batch : 001, train_loss : 0.4802, validation_loss : 0.4731,  0:00:00 &lt; 0:00:00                    epoch : 0070/0080, batch : 002, train_loss : 0.4820, validation_loss : 0.4731,  0:00:00 &lt; 0:00:00                    epoch : 0070/0080, batch : 003, train_loss : 0.4538, validation_loss : 0.4731,  0:00:00 &lt; 0:00:00                    epoch : 0070/0080, batch : 004, train_loss : 0.5165, validation_loss : 0.4731,  0:00:00 &lt; 0:00:00                    epoch : 0071/0080, batch : 001, train_loss : 0.4742, validation_loss : 0.4674,  0:00:00 &lt; 0:00:00                    epoch : 0071/0080, batch : 002, train_loss : 0.4764, validation_loss : 0.4674,  0:00:00 &lt; 0:00:00                    epoch : 0071/0080, batch : 003, train_loss : 0.4480, validation_loss : 0.4674,  0:00:00 &lt; 0:00:00                    epoch : 0071/0080, batch : 004, train_loss : 0.5107, validation_loss : 0.4674,  0:00:00 &lt; 0:00:00                    epoch : 0072/0080, batch : 001, train_loss : 0.4684, validation_loss : 0.4618,  0:00:00 &lt; 0:00:00                    epoch : 0072/0080, batch : 002, train_loss : 0.4710, validation_loss : 0.4618,  0:00:00 &lt; 0:00:00                    epoch : 0072/0080, batch : 003, train_loss : 0.4424, validation_loss : 0.4618,  0:00:00 &lt; 0:00:00                    epoch : 0072/0080, batch : 004, train_loss : 0.5049, validation_loss : 0.4618,  0:00:00 &lt; 0:00:00                    epoch : 0073/0080, batch : 001, train_loss : 0.4627, validation_loss : 0.4563,  0:00:00 &lt; 0:00:00                    epoch : 0073/0080, batch : 002, train_loss : 0.4657, validation_loss : 0.4563,  0:00:00 &lt; 0:00:00                    epoch : 0073/0080, batch : 003, train_loss : 0.4369, validation_loss : 0.4563,  0:00:00 &lt; 0:00:00                    epoch : 0073/0080, batch : 004, train_loss : 0.4993, validation_loss : 0.4563,  0:00:00 &lt; 0:00:00                    epoch : 0074/0080, batch : 001, train_loss : 0.4571, validation_loss : 0.4509,  0:00:00 &lt; 0:00:00                    epoch : 0074/0080, batch : 002, train_loss : 0.4605, validation_loss : 0.4509,  0:00:00 &lt; 0:00:00                    epoch : 0074/0080, batch : 003, train_loss : 0.4315, validation_loss : 0.4509,  0:00:00 &lt; 0:00:00                    epoch : 0074/0080, batch : 004, train_loss : 0.4938, validation_loss : 0.4509,  0:00:00 &lt; 0:00:00                    epoch : 0075/0080, batch : 001, train_loss : 0.4517, validation_loss : 0.4456,  0:00:00 &lt; 0:00:00                    epoch : 0075/0080, batch : 002, train_loss : 0.4554, validation_loss : 0.4456,  0:00:00 &lt; 0:00:00                    epoch : 0075/0080, batch : 003, train_loss : 0.4262, validation_loss : 0.4456,  0:00:00 &lt; 0:00:00                    epoch : 0075/0080, batch : 004, train_loss : 0.4883, validation_loss : 0.4456,  0:00:00 &lt; 0:00:00                    epoch : 0076/0080, batch : 001, train_loss : 0.4463, validation_loss : 0.4404,  0:00:00 &lt; 0:00:00                    epoch : 0076/0080, batch : 002, train_loss : 0.4504, validation_loss : 0.4404,  0:00:00 &lt; 0:00:00                    epoch : 0076/0080, batch : 003, train_loss : 0.4210, validation_loss : 0.4404,  0:00:00 &lt; 0:00:00                    epoch : 0076/0080, batch : 004, train_loss : 0.4829, validation_loss : 0.4404,  0:00:00 &lt; 0:00:00                    epoch : 0077/0080, batch : 001, train_loss : 0.4411, validation_loss : 0.4353,  0:00:00 &lt; 0:00:00                    epoch : 0077/0080, batch : 002, train_loss : 0.4455, validation_loss : 0.4353,  0:00:00 &lt; 0:00:00                    epoch : 0077/0080, batch : 003, train_loss : 0.4159, validation_loss : 0.4353,  0:00:00 &lt; 0:00:00                    epoch : 0077/0080, batch : 004, train_loss : 0.4777, validation_loss : 0.4353,  0:00:00 &lt; 0:00:00                    epoch : 0078/0080, batch : 001, train_loss : 0.4359, validation_loss : 0.4302,  0:00:00 &lt; 0:00:00                    epoch : 0078/0080, batch : 002, train_loss : 0.4407, validation_loss : 0.4302,  0:00:00 &lt; 0:00:00                    epoch : 0078/0080, batch : 003, train_loss : 0.4109, validation_loss : 0.4302,  0:00:00 &lt; 0:00:00                    epoch : 0078/0080, batch : 004, train_loss : 0.4725, validation_loss : 0.4302,  0:00:00 &lt; 0:00:00                    epoch : 0079/0080, batch : 001, train_loss : 0.4309, validation_loss : 0.4253,  0:00:00 &lt; 0:00:00                    epoch : 0079/0080, batch : 002, train_loss : 0.4360, validation_loss : 0.4253,  0:00:00 &lt; 0:00:00                    epoch : 0079/0080, batch : 003, train_loss : 0.4060, validation_loss : 0.4253,  0:00:00 &lt; 0:00:00                    epoch : 0079/0080, batch : 004, train_loss : 0.4674, validation_loss : 0.4253,  0:00:00 &lt; 0:00:00                    epoch : 0080/0080, batch : 001, train_loss : 0.4259, validation_loss : 0.4205,  0:00:00 &lt; 0:00:00                    epoch : 0080/0080, batch : 002, train_loss : 0.4315, validation_loss : 0.4205,  0:00:00 &lt; 0:00:00                    epoch : 0080/0080, batch : 003, train_loss : 0.4012, validation_loss : 0.4205,  0:00:00 &lt; 0:00:00                    epoch : 0080/0080, batch : 004, train_loss : 0.4624, validation_loss : 0.4205,  0:00:00 &lt; 0:00:00                    ...training complete !!
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_sample</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_class_result</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_rsquare_rmse</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_get_time_estimation</td>
          <td class="col-duration">2.20</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_paramOptimizer</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_get_category_edges</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_grouped_mode</td>
          <td class="col-duration">0.00</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_feature_evaluator</td>
          <td class="col-duration">0.28</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>Evaluation for label : Close (Numerical label) ....  done
<br/></div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_Filters</td>
          <td class="col-duration">0.69</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="empty log">No log output captured.</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">tests/test_msd.py::test_moving_slope</td>
          <td class="col-duration">0.62</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log"> ------------------------------Captured stdout call------------------------------ <br/>  0 percent completed...  2 percent completed...  3 percent completed...  4 percent completed...  5 percent completed...  6 percent completed...  7 percent completed...  8 percent completed...  9 percent completed... 10 percent completed... 11 percent completed... 12 percent completed... 13 percent completed... 14 percent completed... 15 percent completed... 16 percent completed... 17 percent completed... 18 percent completed... 19 percent completed... 20 percent completed... 21 percent completed... 22 percent completed... 23 percent completed... 24 percent completed... 25 percent completed... 26 percent completed... 27 percent completed... 28 percent completed... 29 percent completed... 30 percent completed... 31 percent completed... 32 percent completed... 33 percent completed... 34 percent completed... 35 percent completed... 36 percent completed... 37 percent completed... 38 percent completed... 39 percent completed... 40 percent completed... 41 percent completed... 42 percent completed... 43 percent completed... 44 percent completed... 45 percent completed... 46 percent completed... 47 percent completed... 48 percent completed... 49 percent completed... 50 percent completed... 51 percent completed... 52 percent completed... 53 percent completed... 54 percent completed... 55 percent completed... 56 percent completed... 57 percent completed... 58 percent completed... 59 percent completed... 60 percent completed... 61 percent completed... 62 percent completed... 63 percent completed... 64 percent completed... 65 percent completed... 66 percent completed... 67 percent completed... 68 percent completed... 69 percent completed... 70 percent completed... 71 percent completed... 72 percent completed... 73 percent completed... 74 percent completed... 75 percent completed... 76 percent completed... 77 percent completed... 78 percent completed... 79 percent completed... 80 percent completed... 81 percent completed... 82 percent completed... 83 percent completed... 84 percent completed... 85 percent completed... 86 percent completed... 87 percent completed... 88 percent completed... 89 percent completed... 90 percent completed... 91 percent completed... 92 percent completed... 93 percent completed... 94 percent completed... 95 percent completed... 96 percent completed... 97 percent completed... 98 percent completed... 99 percent completed...100 percent completed...101 percent completed...102 percent completed...100 percent completed... !!
<br/></div></td></tr></tbody></table></body></html>